I"I!<ul>
  <li>바로 전 섹션에서 Gaussian random vector의 빈 부분을 채우는 방법을 공부했다.</li>
  <li>이제 그것을 넘어, <strong>noisy observation</strong>이 있는 경우 어떻게 해야 하는 지를 알아보자.</li>
  <li>우선, \( \mathbf{z} \in \mathbb{R}^D \)가 unknown vector value이고, \( \mathbf{y} \in \mathbb{R}^K \) 가 noisy observation이라고 가정하자.</li>
</ul>

\[\begin{align}
p(\mathbf{z}) &amp;= \mathcal{N}(\boldsymbol{\mu}_z, \mathbf{\Sigma}_z) \qquad{(3.98)}\\
p(\mathbf{y}|\mathbf{z}) &amp;= \mathcal{N}(\mathbf{y}|\mathbf{W}\mathbf{z} + \mathbf{b}, \mathbf{\Sigma}_y) \qquad{(3.99)}
\end{align}\]

<ul>
  <li>위와 같이 \( \mathbf{y} \)는 \( \mathbf{z} \) 에 dependent하며, \( \mathbf{W} \in \mathbb{R}^{D \times K}\)matrix를 통해 관계식이 정립된다.</li>
  <li>\( p(\mathbf{z}, \mathbf{y}) = p(\mathbf{z})p(\mathbf{y}|\mathbf{z}) \)의 joint distribution의 평균과 분산은 아래와 같이 정의되며, 증명은 따로 정리해두었다. <a href="/Murphy_PML/solutions/LGS.pdf">proof</a></li>
</ul>

\[\begin{align}
\boldsymbol{\mu} &amp;= \begin{pmatrix} \boldsymbol{\mu}_z \\ \mathbf{W}\boldsymbol{\mu} + \mathbf{b} \end{pmatrix} \qquad{(3.100)}\\
\boldsymbol{\Sigma} &amp;= \begin{pmatrix} \boldsymbol{\Sigma}_z &amp; \boldsymbol{\Sigma}_z \mathbf{W}^\top \\ \mathbf{W}\boldsymbol{\Sigma}_z &amp; \boldsymbol{\Sigma}_x + \mathbf{W}\boldsymbol{\Sigma}_z \mathbf{W}^\top \end{pmatrix} \qquad{(3.101)}
\end{align}\]

<ul>
  <li>마찬가지로, Bayes rule을 이용해서 posterior \( p(\mathbf{z}|\mathbf{y}) \)에 대한 parameter 값도 analytic하게 구할 수 있으며, 위 증명에 같이 정리되어있다.
    <ul>
      <li>이를 <strong>Bayes’ rule for Gaussians</strong> 라고 한다.</li>
    </ul>
  </li>
</ul>

\[\begin{align}
p(\mathbf{z}|\mathbf{y}) &amp;= \frac{p(\mathbf{y}|\mathbf{z})p(\mathbf{z})}{p(\mathbf{y})} = \mathcal{N}(\mathbf{z}|\boldsymbol{\mu}_{z|y},\boldsymbol{\Sigma}_{z|y}) \qquad{(3.102)}\\
\boldsymbol{\Sigma}_{z|y}^{-1} &amp;= \boldsymbol{\Sigma}_z^{-1} + \mathbf{W}^\top \boldsymbol{\Sigma}_y^{-1}\mathbf{W} \qquad{(3.103)}\\
\boldsymbol{\mu}_{z|y} &amp;= \boldsymbol{\Sigma}_{z|y}[\mathbf{W}^\top \boldsymbol{\Sigma}_y^{-1}(\mathbf{y} - \mathbf{b}) + \boldsymbol{\Sigma}_z^{-1} \boldsymbol{\mu}_z] \qquad{(3.104)}
\end{align}\]

<ul>
  <li>마찬가지로, normalization constant \( p(\mathbf{y}) \)도 Gaussian의 형태로 정리가 되며, 따라서 <strong>prior, posterior, marginal 모두 Gaussian</strong>이라는 것을 알 수 있다.
    <blockquote>
      <p>Gaussian prior 과 Gaussian likelihood가 합해지면 Gaussian posterior를 얻을 수 있고, 이와 같이 같은 형태의 likelihood/posterior를 얻을 수 있게 해주는 likelihood를 <strong>conjugate prior</strong>라고 한다.</p>
    </blockquote>
  </li>
</ul>

<h2 id="1-example-inferring-a-latent-vector-from-a-noisy-sensor">1. Example: inferring a latent vector from a noisy sensor</h2>

<ul>
  <li>우리가 알고자 하는 벡터 \( \mathbf{z} \in \mathbb{R}^D \)가 있다고 하자.
    <ul>
      <li>이에 대한 prior는 Gaussian prior \( p(\mathbf{z}) = \mathcal{N}(\boldsymbol{\mu}_z, \boldsymbol{\Sigma}_z) \)로 주어진다.</li>
      <li>만약 아는게 아무것도 없다면, \( \boldsymbol{\Sigma}_z = \infty \mathbf{I} \)로 설정하는 것이 합리적일 것이다.</li>
      <li>아는게 아무것도 없으므로, 평균 또한 0으로 두자.</li>
    </ul>
  </li>
  <li>\( \mathbf{z} \)에 대한 noisy observation \( \mathbf{y}_n \)을 N번 샘플링 했다고 생각하자. (dimension K는 오타인듯..?)
\(p(\mathcal{D}|\mathbf{z}) = \prod_{n=1}^{N} \mathcal{N}(\mathbf{y}_n|\mathbf{z}, \boldsymbol{\Sigma}_y) = \mathcal{N}(\bar{\mathbf{y}}|\mathbf{z}, \frac{1}{N}\boldsymbol{\Sigma}_y) \qquad{(3.40)}\)</li>
  <li>위 식을 정리할 때 \( N \) 개의 observation 평균 \( \bar{\mathbf{y}} \)을 이용하여 대체했고, 그에 맞게 covariance matrix를 \( N \)으로 나누어주는 trick을 이용했다.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>여기서 \( \mathbf{W} = \mathbf{I}, \mathbf{b} = \mathbf{0} \)를 적용하면, 위에서 정리한 Bayes rule for Gaussian 식을 그대로 적용할 수 있고, posterior distribution \( p(\mathbf{z}</td>
          <td>\mathcal{D}) \)를 얻을 수 있다.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>
<div class="text-center">
  <img src="/Murphy_PML/images/3.14.PNG" alt="Figure 3.14" height="300px" />
</div>
<ul>
  <li>위 그림은 이러한 linear Gaussian system의 2D 예제를 보여준다.
    <ul>
      <li>\( \mathbf{z} \): 2d 공간의 좌표</li>
      <li>\( \mathbf{y}_n \): 좌표의 noisy observation</li>
    </ul>
  </li>
  <li>\( \mathbf{y}_n \)을 더 많이 관측할수록, 실제 값 \( \mathbf{z} \)을 더 쉽게 유추할 수 있게 된다.
    <ul>
      <li>이를 temporal axis로까지 연장한 알고리즘을 <strong>Kalman filter</strong> 알고리즘이라고 한다.</li>
    </ul>
  </li>
  <li>위 그림의 맨 오른쪽을 보면 posterior distribution이 어떻게 형성되었는지를 볼 수 있다. \( \mathbf{z} \)의 각 좌표 중 더 많이 퍼진 horizontal axis가 uncertainty가 더 높고, 따라서 해당 sensor에 대한 reliability가 떨어진다고 해석할 수 있다.</li>
</ul>

<h2 id="2-example-inferring-a-latent-vector-from-multiple-noisy-sensors">2. Example: inferring a latent vector from multiple noisy sensors</h2>

<ul>
  <li>이제 바로 앞선 섹션에서 풀었던 noisy sensor문제를 <strong>sensor가 여러개</strong> 인 상황에서 가정해보자.</li>
</ul>

\[p(\mathbf{z}, \mathbf{y}) = p(\mathbf{z})\prod_{m=1}^M \prod_{n=1}^{N_m} \mathcal{N}(\mathbf{y}_{n,m}|\mathbf{z},\boldsymbol{\Sigma}_m) \qquad{(3.44)}\]

<ul>
  <li>파라미터들은 다음과 같이 정의된다.
    <ul>
      <li>\( M \): 센서의 갯수</li>
      <li>\( N_m \): 센서 \( m \)에서 observation의 갯수</li>
    </ul>
  </li>
  <li>따라서, M개의 서로 다른 센서에서 얻은 N개의 observation들로 posterior \( p(\mathbf{z}|\mathbf{y}) \)를 추론해내는 것이 우리의 목표이다.
    <ul>
      <li>\( \mathbf{y}_1 \sim \mathcal{N}(\mathbf{z},\boldsymbol{\Sigma}_1) \)</li>
      <li>\( \mathbf{y}_2 \sim \mathcal{N}(\mathbf{z},\boldsymbol{\Sigma}_2) \)</li>
      <li>위와 같은 두 센서로부터 데이터를 얻게 되며, 이를 concatenation해서 하나의 벡터 \( \mathbf{y} = [\mathbf{y}_1, \mathbf{y}_2] \)로 나타낼 수 있다.</li>
    </ul>
  </li>
  <li>\( p(\mathbf{y}|\mathbf{z}) = \mathcal{N}(\mathbf{y}|\mathbf{W}\mathbf{z}, \boldsymbol{\Sigma}_y) \)와 같은 형태로 likelihood를 표현할 수 있게 되는데, 이 때
    <ul>
      <li>\( \mathbf{W} = [\mathbf{I}; \mathbf{I}] \)</li>
      <li>\( \boldsymbol{\Sigma}_y = [\boldsymbol{\Sigma}_1, \mathbf{0}; \mathbf{0}, \boldsymbol{\Sigma}_2]) \)</li>
    </ul>
  </li>
  <li>이제 식 (3.103)과 식 (3.104)를 바로 이용해 posterior를 구할 수 있다.</li>
  <li>일단 \( \boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_2 = 0.01 \mathbf{I} \) 인 경우를 예로 들어보자.
    <ul>
      <li>추가적인 assumption으로, \( \boldsymbol{\Sigma}_z = 0.01 \mathbf{I} \)도 가정하자.</li>
      <li>아래와 같은 결과를 얻는다.</li>
    </ul>
  </li>
</ul>

\[\begin{align}
\boldsymbol{\Sigma}_{z|y}^{-1} &amp;= \boldsymbol{\Sigma}_{z}^{-1} + \begin{pmatrix}
\mathbf{I} \\ \mathbf{I}\end{pmatrix}^\top \begin{pmatrix} \boldsymbol{\Sigma}_{1}^{-1} &amp; \mathbf{0} \\ \mathbf{0} &amp; \boldsymbol{\Sigma}_2^{-1}\end{pmatrix} \begin{pmatrix} \mathbf{I} \\ \mathbf{I} \end{pmatrix}\\
&amp;= \boldsymbol{\Sigma}_z^{-1} + \boldsymbol{\Sigma}_1^{-1} + \boldsymbol{\Sigma}_2^{-1} \\
&amp;= 300 \mathbf{I}
\end{align}\]

\[\begin{align}
\boldsymbol{\mu}_{z|y} &amp;= 
\boldsymbol{\Sigma}_{z|y} \begin{pmatrix}
\mathbf{I} \\ \mathbf{I} \end{pmatrix}^\top \begin{pmatrix} \boldsymbol{\Sigma}_1^{-1} &amp; \mathbf{0} \\ \mathbf{0} &amp; \boldsymbol{\Sigma}_2^{-1} \end{pmatrix} \begin{pmatrix} \mathbf{y}_1 \\ \mathbf{y}_2 \end{pmatrix}\\
&amp;= 0.03 \mathbf{I} (100 \mathbf{y}_1 + 100 \mathbf{y}_2)
\end{align}\]

<ul>
  <li>결국 분산값은 각 sensor에 대해 얼마나 신뢰도가 있었는지에 따라 additive하게 증가하게 되고, 평균값은 각 sensor 평균을 신뢰도에 따라 weighted sum을 한 결과가 나오게 된다.</li>
</ul>
:ET