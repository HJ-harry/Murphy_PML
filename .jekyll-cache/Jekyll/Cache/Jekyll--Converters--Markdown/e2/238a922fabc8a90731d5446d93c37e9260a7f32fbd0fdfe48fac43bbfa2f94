I"‚-<ul>
  <li>Joint probability distributionìœ¼ë¡œ ê°€ì¥ ë§ì´ í”íˆ ì´ìš©ë˜ëŠ”ê²ƒì´ <strong>multivariate gaussian distribution</strong> í˜¹ì€ <strong>multivariate normal(MVN) distribution</strong>ì´ë‹¤.</li>
  <li>ìˆ˜í•™ì ìœ¼ë¡œ ë‹¤ë£¨ê¸° í¸í•´ì„œ ê·¸ëŸ°ê²ƒë„ ìˆì§€ë§Œ, ì•ì„  ì´ìœ ë“¤ ë•Œë¬¸ì— í•©ë¦¬ì ì´ê¸°ë„ í•˜ë‹¤.</li>
  <li>PRMLì˜ 2ë‹¨ì›ì— MVNì— ê´€í•´ êµ‰ì¥íˆ ìì„¸íˆ ë‹¤ë£¨ëŠ”ë°, ì •ë¦¬ê°€ ì˜ ë˜ì–´ìˆìœ¼ë‹ˆ í•œ ë²ˆ ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤.</li>
</ul>

<h2 id="1-definition">1. Definition</h2>

\[\mathcal{N}(\mathbf{y}|\boldsymbol{\mu},\mathbf{\Sigma}) := \frac{1}{(2\pi)^{D/2}|\mathbf{\Sigma}|^{1/2}} \exp \Big[ -\frac{1}{2}(\mathbf{y} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{y} - \boldsymbol{\mu}) \Big] \qquad{(3.77)}\]

<ul>
  <li>\( D \)-dimensionì¼ ë•Œ ìœ„ ì‹ê³¼ ê°™ì´ ì •ì˜ëœë‹¤.
    <ul>
      <li>\( \boldsymbol{\mu} = \mathbb{E}[\mathbf{y}] \in \mathbb{R}^D \): mean vector</li>
      <li>\( \mathbf{\Sigma} = \mathrm{Cov}[\mathbf{y}] \): \( D \times D \) covariance matrix</li>
      <li>ì—¬ê¸°ì„œ <strong>covariance matrix</strong>ë€ \( \mathrm{Cov}[\mathbf{y}] := \mathbb{E}\Big[ (y - \mathbb{E}[\mathbf{y}])(y - \mathbb{E}[\mathbf{y}])^T \Big]\)</li>
    </ul>
  </li>
</ul>

\[\begin{align}
\mathrm{Cov}[Y_i, Y_j] &amp;= \mathbb{E}[(Y_i - \mathbb{E}[Y_i])(Y_j - \mathbb{E}[Y_j])]\\
&amp;= \mathbb{E}[Y_i Y_j] - \mathbb{E}[Y_i\mathbb{E}[Y_j]] - \mathbb{E}[Y_j\mathbb{E}[Y_i]] + \mathbb{E}[Y_i]\mathbb{E}[Y_j]
\end{align}\]

<ul>
  <li>Covariance matrixì˜ (i, j) ë²ˆì§¸ entryëŠ” ìœ„ì™€ ê°™ë‹¤.
    <ul>
      <li>ë”°ë¼ì„œ diagonal elementë“¤ì€ \( \mathbb{V}[Y_i] = \mathrm{Cov}[Y_i, Y_i] \) ì´ë‹¤.</li>
    </ul>
  </li>
  <li>Covariance matrixì˜ ì •ì˜ë¡œë¶€í„°</li>
</ul>

\[\begin{align}
\mathbf{\Sigma} &amp;= \mathbb{E}[(\mathbf{y} - \mathbb{E}[\mathbf{y}])(\mathbf{y} - \mathbb{E}[\mathbf{y}])^T]]\\
&amp;= \mathbb{E}[\mathbf{y}\mathbf{y}^T - \boldsymbol{\mu}\mathbf{y}T - \mathbf{y}\boldsymbol{\mu}^T + \mu \mu^T]\\
&amp;= \mathbb{E}[\mathbf{y}\mathbf{y}^T] - \boldsymbol{\mu}\boldsymbol{\mu}^T \qquad{(3.81)}
\end{align}\]

<ul>
  <li>exponential term ì•ì— ë¶™ëŠ” \( Z = (2\pi)^{D/2}|\mathbf{\Sigma}|^{1/2} \)ëŠ” normalization constantë¡œ ì ë¶„ê°’ì„ 1ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.</li>
  <li>\( D=2 \)ì¼ ë•Œë¥¼ <strong>bivariate gaussian</strong>ì´ë¼ê³  í•˜ë©°, covariance matrixê°€ ë‹¬ë¼ì§ì— ë”°ë¼ ì„œë¡œ ë‹¤ë¥¸ ëª¨ì–‘ì„ ê°–ëŠ”ê²ƒì´ ì•„ë˜ ê·¸ë¦¼ë“¤ì— í‘œí˜„ë˜ì–´ìˆë‹¤.</li>
</ul>
<div class="text-center">
  <img src="/Murphy_PML/images/3.11.PNG" alt="Figure 3.11" height="300px" />
</div>
<div class="text-center">
  <img src="/Murphy_PML/images/3.12.PNG" alt="Figure 3.12" height="300px" />
</div>
<ul>
  <li>Covariance matrixëŠ” ì •ì˜ì— ë”°ë¼ symmetricí•˜ë¯€ë¡œ
    <ul>
      <li><strong>full covariance matrix</strong>ëŠ” ìµœëŒ€ \( D(D+1)/2 \)ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°–ëŠ”ë‹¤.
        <ul>
          <li>ì´ ê²½ìš°ì— ê°€ì¥ ììœ ë„ê°€ ë†’ìœ¼ë¯€ë¡œ ììœ ë¶„ë°©í•˜ê²Œ ìƒê¸´(?) íƒ€ì› level setì„ ìƒê°í•  ìˆ˜ ìˆë‹¤.</li>
          <li>3.11ê³¼ 3.12ì˜ (a)</li>
        </ul>
      </li>
      <li><strong>Diagonal covariance matrix</strong>ëŠ” \( Cov[Y_i, Y_j] = 0 \, i \neq j \)ì¸ ê²½ìš°ì´ë©° Dê°œì˜ íŒŒë¼ë¯¸í„° ê°’ì„ ê°–ëŠ”ë‹¤.
        <ul>
          <li>Diagonal matrixê°€ í˜•ì„±ë˜ë©°, ê° axisì— ìˆ˜ì§í•œ íƒ€ì› level setì„ ê°–ëŠ”ë‹¤.</li>
          <li>3.11ê³¼ 3.12ì˜ (b)</li>
        </ul>
      </li>
      <li><strong>Isotropic covariance matrix</strong>ëŠ” varianceê°’ìœ¼ë¡œ scaleëœ identity matrix.
        <ul>
          <li>3.11ê³¼ 3.12ì˜ (c)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-mahalanobis-distance">2. Mahalanobis distance</h2>

<ul>
  <li>Normalization constant \( Z \)ë¥¼ ì œì™¸í•˜ë©´ ê²°êµ­ gaussianì€ exponential functionì˜ í˜•íƒœë¥¼ ëˆë‹¤.</li>
  <li>ì´ë¥¼ ë‹¤ë£¨ê¸° í¸í•˜ê²Œ í•˜ê¸° ìœ„í•´ logë¥¼ ì·¨í•˜ê³  ì§€ìˆ˜ ë¶€ë¶„ë§Œì„ ë³´ëŠ” ê²½ìš°ê°€ ë§ì€ë°,</li>
</ul>

\[\log p(\mathbf{y}|\boldsymbol{\mu}, \mathbf{\Sigma}) = -\frac{1}{2}(\mathbf{y} - \boldsymbol{\mu}^T)\mathbf{\Sigma}^{-1}(\mathbf{y} - \boldsymbol{\mu}) + \mathrm{(const)} \qquad{(3.85)}\]

<ul>
  <li>ìœ„ ì‹ê³¼ ê°™ì´ ì£¼ì–´ì§„ë‹¤. ì—¬ê¸°ì„œ <strong>Mahalanobis distance</strong>ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•œë‹¤.</li>
</ul>

\[\Delta^2 := (\mathbf{y} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{y} - \boldsymbol{\mu}) \qquad{(3.86)}\]

<ul>
  <li>ê·¸ë¦¼ 3.12ì—ì„œ ë³´ì•˜ë˜ level setì„ ìƒê°í•´ë³´ë©´, ê°™ì€ level setì€ ê²°êµ­ ê°™ì€ \( \Delta \) ê°’ì„ ê°–ëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤.</li>
  <li>Covariance ê°’ì€ non-negativeì´ê³ , ë”°ë¼ì„œ <strong>positive semi-definite (PSD)</strong> matrixì´ë‹¤.
    <ul>
      <li>ì±…ì—ëŠ” positive definiteì´ë¼ê³  ë‚˜ì™€ìˆê³  full rankë¥¼ ê°€ì •í•˜ëŠ” ê²ƒ ê°™ì€ë°, ì¤‘ìš”í•œ ì»¨ì…‰ì€ ê·¸ê²Œ ì•„ë‹ˆë‹ˆ ë„˜ì–´ê°€ì.</li>
    </ul>
  </li>
  <li>Symmetric PSD matrixëŠ” í•´ë‹¹ matrixë‚˜ ê·¸ matrixì˜ inverseì¸ \(\mathbf{\Lambda} = \mathbf{\Sigma}^{-1} \) ëª¨ë‘ diagonalizable í•˜ë‹¤.</li>
  <li>ë”°ë¼ì„œ ì´ë¥¼ eigendecompositioní•´ì„œ í‘œí˜„í•˜ë©´</li>
</ul>

\[\mathbf{\Sigma} = \sum_{d=1}^D \lambda_d \mathbf{u}_d \mathbf{u}_d^T \qquad{(3.87)}\]

\[\mathbf{\Sigma}^{-1} = \sum_{d=1}^D \frac{1}{\lambda_d} \mathbf{u}_d \mathbf{u}_d^T \qquad{(3.88)}\]

<p>-ì´ë ‡ê²Œ eigenvectorì™€ eigenvalueë¡œ Mahalanobis distanceë¥¼ í‘œí˜„í•˜ê³ , ì¶”ê°€ì ìœ¼ë¡œ \( z_d := \mathbf{u}_d^T (\mathbf{y} - \boldsymbol{\mu})\), ë”°ë¼ì„œ \( \mathbf{z} = \mathbf{U}(\mathbf{y} - \boldsymbol{\mu}) \)ë¥¼ ì´ìš©í•˜ë©´,</p>

\[\begin{align}
\Delta^2 &amp;= (\mathbf{y} - \boldsymbol{\mu})^T (\sum_{d=1}^D \frac{1}{\lambda_d} \mathbf{u}_d \mathbf{u}_d^T) (\mathbf{y} - \boldsymbol{\mu}) \\
&amp;= \sum_{d=1}^D \frac{1}{\lambda_d}(\mathbf{y} - \boldsymbol{\mu})^T \mathbf{u}_d \mathbf{u}_d^T (\mathbf{y} - \boldsymbol{\mu}) = \sum_{d=1}^D \frac{z_d^2}{\lambda_d}
\end{align}\]

<ul>
  <li>ì´ë ‡ê²Œ ì‹ì„ ë°”ê¿”ì„œ í•´ì„í•´ë³´ë©´, ê²°êµ­ Mahalanobis distanceê°€ ê°™ë‹¤ëŠ” ê²ƒì€ ìƒˆë¡œìš´ ì¢Œí‘œê³„ì¸ \( \mathbf{z} \) ì¢Œí‘œê³„ì—ì„œ Euclidean distanceê°€ ê°™ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
    <ul>
      <li>ê·¸ë¦¬ê³  ì´ ìƒˆë¡œìš´ ì¢Œí‘œê³„ëŠ” \( \mathbf{y} \)ë¥¼ rotation matrix \( \mathbf{U} \)ë¡œ ëŒë¦¬ê³  \( \mathbf{\Lambda} \)ë¡œ scaleí•´ì„œ ì–»ì–´ì§„ë‹¤.
-2-d caseë¡œ ì˜ˆë¥¼ ë“¤ìë©´
\(\frac{z_1^2}{\lambda_1} + \frac{z_2^2}{\lambda_2} = r \qquad{(3.91)}\)</li>
    </ul>
  </li>
  <li>ìœ„ì— ìˆëŠ” ì ë“¤ì€ ëª¨ë‘ ê°™ì€ Mahalanobis distanceë¥¼ ê°–ëŠ”ë°, ì´ ëª¨ì–‘ì€ íƒ€ì›ì˜ í˜•íƒœë¥¼ ë„ë¯€ë¡œ Figure 3.12ì˜ level setë“¤ì´ íƒ€ì› ëª¨ì–‘ì„ ê°€ì¡Œë˜ ê²ƒì´ë‹¤.</li>
  <li>PSD matrixëŠ” ìŠ¤ì¹¼ë¼ ê°’ì˜ scalar ê°’ì„ êµ¬í•˜ë“¯ì´ \( L^T L \)ì™€ ê°™ì´ factorizationë„ ê°€ëŠ¥í•œë°, \( \mathbf{\Sigma}^{-1} = \mathbf{L}^T \mathbf{L} \)ì„ ì •ì˜í•˜ì.
    <ul>
      <li>ì´ ë•Œ \( \mathbf{L}: \mathbb{R}^D \mapsto \mathbb{R}^d, \quad d \leq D \) ì´ë©°, mahalanobis distanceë¥¼ \( \boldsymbol{\delta} = \mathbf{y} - \boldsymbol{\mu} \)ë¥¼ ì´ìš©í•´ ì „ê°œí•´ë³´ë©´</li>
    </ul>
  </li>
</ul>

\[\Delta^2 = \| \mathbf{L} \boldsymbol{\delta} \|^2_2 \qquad{(3.92)}\]

<ul>
  <li>ìœ„ ì‹ì„ ì–»ëŠ”ë‹¤. ë”°ë¼ì„œ \( D \) ë³´ë‹¤ ì‘ì€ dimensionì¸ \( d \) dimensionì—ì„œì˜ Euclidean distanceë¡œë„ ìƒê°í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<h2 id="3-marginals-and-conditionals-of-an-mvn">3. Marginals and conditionals of an MVN</h2>

<ul>
  <li>ì´ì œ MVNì˜ marginal/conditional distributionì„ ì•Œì•„ë³¼ ê²ƒì¸ë°, ì§ì ‘ ê³„ì‚°í•˜ë©´ì„œ ì•Œì•„ë³´ê¸° ìœ„í•´ 2D discrete case with random variables \( Y_1, Y_2 \) ì¸ ê²½ìš°ë¥¼ ìƒê°í•˜ì.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>\( p(Y_1, Y_2) \)</th>
      <th>\( Y_1 = 0 \)</th>
      <th>\( Y_2 = 1 \)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\( Y_1 = 0 \)</td>
      <td>0.2</td>
      <td>0.3</td>
    </tr>
    <tr>
      <td>\( Y_2 = 1 \)</td>
      <td>0.3</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>\( Y_1, Y_2 \)ì˜ joint distributionì´ ìˆì„ ë•Œ <strong>marginal distribution</strong>ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</li>
</ul>

\[p(Y_2 = y_2) = \sum_{y_1}p(Y_1 = y_1, Y_2 = y_2) \qquad{(3.93)}\]

<ul>
  <li><strong>conditional distribution</strong>ì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ëœë‹¤.</li>
</ul>

\[p(Y_1 = y_1 | Y_2 = y_2) = \frac{p(Y_1 = y_1, Y_2 = y_2)}{p(Y_2 = y_2)} \qquad{(3.94)}\]

<ul>
  <li>ìœ„ ê²°ê³¼ë¥¼ jointly Gaussian random variableì— ëŒ€í•´ í™•ì¥ì‹œì¼œë³´ì.
    <ul>
      <li>\( \mathbf{y}_2 \)ê°€ variable of interestì¼ ë•Œ \( \mathbf{y}_1 \)ì€ <strong>nuisance variable</strong>ì´ë¼ê³  ë¶€ë¥¸ë‹¤.</li>
      <li>ì´ nuisance variableì„ marginalize out í•´ë³´ì</li>
    </ul>
  </li>
</ul>

\[p(\mathbf{y}_2) = \int \mathcal{N} (\mathbf{y}| \boldsymbol{\mu}, \Sigma)\,d\mathbf{y}_1 = \mathcal{N}(\mathbf{y}_2|\boldsymbol{\mu}_2, \mathbf{\Sigma}_{22}) \qquad{(3.95)}\]

<ul>
  <li>ìœ„ ì‹ì—ì„œ ì´ìš©ëœ meanê³¼ covariance matrixëŠ” ì•„ë˜ì™€ ê°™ë‹¤.</li>
</ul>

\[\boldsymbol{\mu} = \begin{pmatrix}\boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2\end{pmatrix}\, , \, \mathbf{\Sigma} =
\begin{pmatrix}
\mathbf{\Sigma}_{11} &amp; \mathbf{\Sigma}_{12} \\
\mathbf{\Sigma}_{21} &amp; \mathbf{\Sigma}_{22}
\end{pmatrix}
\qquad{(3.96)}\]

<ul>
  <li>ì´ì œ ë§Œì•½ \( \mathbf{y}_2 \)ë¥¼ observeí•˜ê³ , ì´ë¥¼ í†µí•´ \( \mathbf{y}_1 \)ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ë‹¤ê³  í•˜ì.</li>
  <li>ì´ëŠ” ê²°êµ­ posteriorì¸ \( p(\mathbf{y}_1 | \mathbf{y}_2) \)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œê°€ ë˜ë©°, ì´ë¥¼ ê³„ì‚°í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ ì‹ì´ ë‚˜ì˜¨ë‹¤.</li>
</ul>

\[p(\mathbf{y}_1 | \mathbf{y}_2) = \mathcal{N}(\mathbf{y}_1|\boldsymbol{\mu}_1 +
\mathbf{\Sigma}_{12} \mathbf{\Sigma}_{22}^{-1}(\mathbf{y}_2 - \boldsymbol{\mu}_2), \, \mathbf{\Sigma}_{11} - \mathbf{\Sigma}_{12}\mathbf{\Sigma}_{22}^{-1}\mathbf{\Sigma}_{21}) \qquad{(3.94)}\]

<ul>
  <li>ìœ„ ì‹ì˜ meanê³¼ varianceë¥¼ ì‚´í‘œë³´ì
    <ul>
      <li>mean: \( \mathbf{y}_2 \)ì— ëŒ€í•œ linear functionì´ë‹¤.</li>
      <li>variance: \( \mathbf{y}_2 \)ì— independentí•˜ë‹¤. ê¸°ì¡´ covarianceì—ë§Œ ì˜í–¥ì„ ë°›ëŠ”ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
    </ul>
  </li>
</ul>

<h2 id="4-example-imputing-missing-values">4. Example: Imputing missing values</h2>

<ul>
  <li>ì´ë²ˆ ì˜ˆì œëŠ” \( \mathbf{y} \) ë²¡í„°ì˜ ì¼ë¶€ë¶„ (ì¼ë¶€ dimension)ë§Œ observeí–ˆë‹¤ê³  ê°€ì •í•˜ê³ , ë‚˜ë¨¸ì§€ missing dimensionì„ covarinace matrixë¡œë¶€í„° ìœ ì¶”í•˜ëŠ” ê³¼ì •ì„ë‹´ëŠ”ë‹¤.
    <ul>
      <li>ì´ë¥¼ <strong>missing value imputation</strong>ì´ë¼ê³  í•œë‹¤.</li>
    </ul>
  </li>
</ul>
<div class="text-center">
  <img src="/Murphy_PML/images/3.13.PNG" alt="Figure 3.13" height="300px" />
</div>
<ul>
  <li>ìœ„ ê·¸ë¦¼ì€ MVNì„ ì´ìš©í•œ missing data imputationì˜ í•œ ì˜ˆì‹œì´ë‹¤.
    <ul>
      <li>ë¹¨ê°„ìƒ‰ì´ í´ìˆ˜ë¡ +ìª½ìœ¼ë¡œ í¬ê³ , ì´ˆë¡ìƒ‰ì´ í´ìˆ˜ë¡ -ìª½ìœ¼ë¡œ í¬ë‹¤.</li>
      <li>ì´ëŸ° ì‹ì˜ visualizationì„ Hinton diagramì´ë¼ê³  í•œë‹¤.</li>
      <li>(c)ëŠ” posterior distributionì„ ê³„ì‚°í•´ mean ê°’ì„ ë‚˜íƒ€ë‚¸ diagramì´ë‹¤. (a)ì— ìˆë˜ ë°ì´í„°ë§Œ ê°–ê³  ìœ ì¶”í•œ ê²ƒ ì¹˜ê³ ëŠ” ground truthì¸ (b)ì— ê°€ê¹Œìš´ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.</li>
    </ul>
  </li>
  <li>ë¨¼ì €, \( N = 8 \)ê°œì˜ \( D = 10 \)dimensional vectorë¥¼ Gaussianì—ì„œ sampling í•œ í›„, ê° vectorì˜ 50%ë¥¼ ì§€ì›Œë²„ë¦°ë‹¤.</li>
  <li>ê·¸ í›„ì—, ìš°ë¦¬ê°€ ì•Œê³ ìˆëŠ” true model parameterë¡œë¶€í„° ë¹„ëŠ” ê°’ì„ ì˜ˆì¸¡í•œë‹¤.
    <ul>
      <li>ë¬¼ë¡  ì‹¤ì œë¡œëŠ” ì´ <em>true parameter</em> ë¼ëŠ” ê°’ë„ ë‹¹ì—°íˆ ìœ ì¶”í•´ì•¼ê² ì§€ë§Œ, ì˜ˆì œì´ë¯€ë¡œ ì•Œê³  ìˆë‹¤ê³  ê°€ì •í•œë‹¤.</li>
    </ul>
  </li>
  <li>ë¬¸ì œë¥¼ formulationí•˜ìë©´ ì´ë ‡ë‹¤.</li>
</ul>

\[p(\mathbf{y}_{n, \mathbf{h}} | \mathbf{y}_{n, \mathbf{v}}, \boldsymbol{\theta})\]
:ET