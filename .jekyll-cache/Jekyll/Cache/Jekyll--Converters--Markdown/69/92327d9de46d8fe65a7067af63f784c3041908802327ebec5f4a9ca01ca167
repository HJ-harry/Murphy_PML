I"®<ul>
  <li>Joint probability distributionìœ¼ë¡œ ê°€ì¥ ë§ì´ í”íˆ ì´ìš©ë˜ëŠ”ê²ƒì´ <strong>multivariate gaussian distribution</strong> í˜¹ì€ <strong>multivariate normal(MVN) distribution</strong>ì´ë‹¤.</li>
  <li>ìˆ˜í•™ì ìœ¼ë¡œ ë‹¤ë£¨ê¸° í¸í•´ì„œ ê·¸ëŸ°ê²ƒë„ ìˆì§€ë§Œ, ì•ì„  ì´ìœ ë“¤ ë•Œë¬¸ì— í•©ë¦¬ì ì´ê¸°ë„ í•˜ë‹¤.</li>
  <li>PRMLì˜ 2ë‹¨ì›ì— MVNì— ê´€í•´ êµ‰ì¥íˆ ìì„¸íˆ ë‹¤ë£¨ëŠ”ë°, ì •ë¦¬ê°€ ì˜ ë˜ì–´ìˆìœ¼ë‹ˆ í•œ ë²ˆ ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤.</li>
</ul>

<h2 id="1-definition">1. Definition</h2>

\[\mathcal{N}(\mathbf{y}|\boldsymbol{\mu},\mathbf{\Sigma}) := \frac{1}{(2\pi)^{D/2}|\mathbf{\Sigma}|^{1/2}} \exp \Big[ -\frac{1}{2}(\mathbf{y} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{y} - \boldsymbol{\mu}) \Big] \qquad{(3.77)}\]

<ul>
  <li>\( D \)-dimensionì¼ ë•Œ ìœ„ ì‹ê³¼ ê°™ì´ ì •ì˜ëœë‹¤.
    <ul>
      <li>\( \boldsymbol{\mu} = \mathbb{E}[\mathbf{y}] \in \mathbb{R}^D \): mean vector</li>
      <li>\( \mathbf{\Sigma} = \mathrm{Cov}[\mathbf{y}] \): \( D \times D \) covariance matrix</li>
      <li>ì—¬ê¸°ì„œ <strong>covariance matrix</strong>ë€ \( \mathrm{Cov}[\mathbf{y}] := \mathbb{E}\Big[ (y - \mathbb{E}[\mathbf{y}])(y - \mathbb{E}[\mathbf{y}])^T \Big]\)</li>
    </ul>
  </li>
</ul>

\[\begin{align}
\mathrm{Cov}[Y_i, Y_j] &amp;= \mathbb{E}[(Y_i - \mathbb{E}[Y_i])(Y_j - \mathbb{E}[Y_j])]\\
&amp;= \mathbb{E}[Y_i Y_j] - \mathbb{E}[Y_i\mathbb{E}[Y_j]] - \mathbb{E}[Y_j\mathbb{E}[Y_i]] + \mathbb{E}[Y_i]\mathbb{E}[Y_j]
\end{align}\]

<ul>
  <li>Covariance matrixì˜ (i, j) ë²ˆì§¸ entryëŠ” ìœ„ì™€ ê°™ë‹¤.
    <ul>
      <li>ë”°ë¼ì„œ diagonal elementë“¤ì€ \( \mathbb{V}[Y_i] = \mathrm{Cov}[Y_i, Y_i] \) ì´ë‹¤.</li>
    </ul>
  </li>
  <li>Covariance matrixì˜ ì •ì˜ë¡œë¶€í„°</li>
</ul>

\[\begin{align}
\mathbf{\Sigma} &amp;= \mathbb{E}[(\mathbf{y} - \mathbb{E}[\mathbf{y}])(\mathbf{y} - \mathbb{E}[\mathbf{y}])^T]]\\
&amp;= \mathbb{E}[\mathbf{y}\mathbf{y}^T - \boldsymbol{\mu}\mathbf{y}T - \mathbf{y}\boldsymbol{\mu}^T + \mu \mu^T]\\
&amp;= \mathbb{E}[\mathbf{y}\mathbf{y}^T] - \boldsymbol{\mu}\boldsymbol{\mu}^T \qquad{(3.81)}
\end{align}\]

<ul>
  <li>exponential term ì•ì— ë¶™ëŠ” \( Z = (2\pi)^{D/2}|\mathbf{\Sigma}|^{1/2} \)ëŠ” normalization constantë¡œ ì ë¶„ê°’ì„ 1ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.</li>
  <li>\( D=2 \)ì¼ ë•Œë¥¼ <strong>bivariate gaussian</strong>ì´ë¼ê³  í•˜ë©°, covariance matrixê°€ ë‹¬ë¼ì§ì— ë”°ë¼ ì„œë¡œ ë‹¤ë¥¸ ëª¨ì–‘ì„ ê°–ëŠ”ê²ƒì´ ì•„ë˜ ê·¸ë¦¼ë“¤ì— í‘œí˜„ë˜ì–´ìˆë‹¤.</li>
</ul>
<div class="text-center">
  <img src="/Murphy_PML/images/3.11.PNG" alt="Figure 3.11" height="300px" />
</div>
<div class="text-center">
  <img src="/Murphy_PML/images/3.12.PNG" alt="Figure 3.12" height="300px" />
</div>
<ul>
  <li>Covariance matrixëŠ” ì •ì˜ì— ë”°ë¼ symmetricí•˜ë¯€ë¡œ
    <ul>
      <li><strong>full covariance matrix</strong>ëŠ” ìµœëŒ€ \( D(D+1)/2 \)ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°–ëŠ”ë‹¤.
        <ul>
          <li>ì´ ê²½ìš°ì— ê°€ì¥ ììœ ë„ê°€ ë†’ìœ¼ë¯€ë¡œ ììœ ë¶„ë°©í•˜ê²Œ ìƒê¸´(?) íƒ€ì› level setì„ ìƒê°í•  ìˆ˜ ìˆë‹¤.</li>
          <li>3.11ê³¼ 3.12ì˜ (a)</li>
        </ul>
      </li>
      <li><strong>Diagonal covariance matrix</strong>ëŠ” \( Cov[Y_i, Y_j] = 0 \, i \neq j \)ì¸ ê²½ìš°ì´ë©° Dê°œì˜ íŒŒë¼ë¯¸í„° ê°’ì„ ê°–ëŠ”ë‹¤.
        <ul>
          <li>Diagonal matrixê°€ í˜•ì„±ë˜ë©°, ê° axisì— ìˆ˜ì§í•œ íƒ€ì› level setì„ ê°–ëŠ”ë‹¤.</li>
          <li>3.11ê³¼ 3.12ì˜ (b)</li>
        </ul>
      </li>
      <li><strong>Isotropic covariance matrix</strong>ëŠ” varianceê°’ìœ¼ë¡œ scaleëœ identity matrix.
        <ul>
          <li>3.11ê³¼ 3.12ì˜ (c)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-mahalanobis-distance">2. Mahalanobis distance</h2>

<ul>
  <li>Normalization constant \( Z \)ë¥¼ ì œì™¸í•˜ë©´ ê²°êµ­ gaussianì€ exponential functionì˜ í˜•íƒœë¥¼ ëˆë‹¤.</li>
  <li>ì´ë¥¼ ë‹¤ë£¨ê¸° í¸í•˜ê²Œ í•˜ê¸° ìœ„í•´ logë¥¼ ì·¨í•˜ê³  ì§€ìˆ˜ ë¶€ë¶„ë§Œì„ ë³´ëŠ” ê²½ìš°ê°€ ë§ì€ë°,</li>
</ul>

\[\log p(\mathbf{y}|\boldsymbol{\mu}, \mathbf{\Sigma}) = -\frac{1}{2}(\mathbf{y} - \boldsymbol{\mu}^T)\mathbf{\Sigma}^{-1}(\mathbf{y} - \boldsymbol{\mu}) + \mathrm{(const)} \qquad{(3.85)}\]

<ul>
  <li>ìœ„ ì‹ê³¼ ê°™ì´ ì£¼ì–´ì§„ë‹¤. ì—¬ê¸°ì„œ <strong>Mahalanobis distance</strong>ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•œë‹¤.</li>
</ul>

\[\Delta^2 := (\mathbf{y} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{y} - \boldsymbol{\mu}) \qquad{(3.86)}\]

<ul>
  <li>ê·¸ë¦¼ 3.12ì—ì„œ ë³´ì•˜ë˜ level setì„ ìƒê°í•´ë³´ë©´, ê°™ì€ level setì€ ê²°êµ­ ê°™ì€ \( \Delta \) ê°’ì„ ê°–ëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤.</li>
  <li>Covariance ê°’ì€ non-negativeì´ê³ , ë”°ë¼ì„œ <strong>positive semi-definite (PSD)</strong> matrixì´ë‹¤.
    <ul>
      <li>ì±…ì—ëŠ” positive definiteì´ë¼ê³  ë‚˜ì™€ìˆê³  full rankë¥¼ ê°€ì •í•˜ëŠ” ê²ƒ ê°™ì€ë°, ì¤‘ìš”í•œ ì»¨ì…‰ì€ ê·¸ê²Œ ì•„ë‹ˆë‹ˆ ë„˜ì–´ê°€ì.</li>
    </ul>
  </li>
  <li>Symmetric PSD matrixëŠ” í•´ë‹¹ matrixë‚˜ ê·¸ matrixì˜ inverseì¸ \(\mathbf{\Lambda} = \mathbf{\Sigma}^{-1} \) ëª¨ë‘ diagonalizable í•˜ë‹¤.</li>
  <li>ë”°ë¼ì„œ ì´ë¥¼ eigendecompositioní•´ì„œ í‘œí˜„í•˜ë©´</li>
</ul>

\[\mathbf{\Sigma} = \sum_{d=1}^D \lambda_d \mathbf{u}_d \mathbf{u}_d^T \qquad{(3.87)}\]

\[\mathbf{\Sigma}^{-1} = \sum_{d=1}^D \frac{1}{\lambda_d} \mathbf{u}_d \mathbf{u}_d^T \qquad{(3.88)}\]

<p>-ì´ë ‡ê²Œ eigenvectorì™€ eigenvalueë¡œ Mahalanobis distanceë¥¼ í‘œí˜„í•˜ê³ , ì¶”ê°€ì ìœ¼ë¡œ \( z_d := \mathbf{u}_d^T (\mathbf{y} - \boldsymbol{\mu})\), ë”°ë¼ì„œ \( \mathbf{z} = \mathbf{U}(\mathbf{y} - \boldsymbol{\mu}) \)ë¥¼ ì´ìš©í•˜ë©´,</p>

\[\begin{align}
\Delta^2 &amp;= (\mathbf{y} - \boldsymbol{\mu})^T (\sum_{d=1}^D \frac{1}{\lambda_d} \mathbf{u}_d \mathbf{u}_d^T) (\mathbf{y} - \boldsymbol{\mu}) \\
&amp;= \sum_{d=1}^D \frac{1}{\lambda_d}(\mathbf{y} - \boldsymbol{\mu})^T \mathbf{u}_d \mathbf{u}_d^T (\mathbf{y} - \boldsymbol{\mu}) = \sum_{d=1}^D \frac{z_d^2}{\lambda_d}
\end{align}\]

<ul>
  <li>ì´ë ‡ê²Œ ì‹ì„ ë°”ê¿”ì„œ í•´ì„í•´ë³´ë©´, ê²°êµ­ Mahalanobis distanceê°€ ê°™ë‹¤ëŠ” ê²ƒì€ ìƒˆë¡œìš´ ì¢Œí‘œê³„ì¸ \( \mathbf{z} \) ì¢Œí‘œê³„ì—ì„œ Euclidean distanceê°€ ê°™ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
    <ul>
      <li>ê·¸ë¦¬ê³  ì´ ìƒˆë¡œìš´ ì¢Œí‘œê³„ëŠ” \( \mathbf{y} \)ë¥¼ rotation matrix \( \mathbf{U} \)ë¡œ ëŒë¦¬ê³  \( \mathbf{\Lambda} \)ë¡œ scaleí•´ì„œ ì–»ì–´ì§„ë‹¤.
-2-d caseë¡œ ì˜ˆë¥¼ ë“¤ìë©´
\(\frac{z_1^2}{\lambda_1} + \frac{z_2^2}{\lambda_2} = r \qquad{(3.91)}\)</li>
    </ul>
  </li>
  <li>ìœ„ì— ìˆëŠ” ì ë“¤ì€ ëª¨ë‘ ê°™ì€ Mahalanobis distanceë¥¼ ê°–ëŠ”ë°, ì´ ëª¨ì–‘ì€ íƒ€ì›ì˜ í˜•íƒœë¥¼ ë„ë¯€ë¡œ Figure 3.12ì˜ level setë“¤ì´ íƒ€ì› ëª¨ì–‘ì„ ê°€ì¡Œë˜ ê²ƒì´ë‹¤.</li>
  <li>PSD matrixëŠ” ìŠ¤ì¹¼ë¼ ê°’ì˜ scalar ê°’ì„ êµ¬í•˜ë“¯ì´ \( L^T L \)ì™€ ê°™ì´ factorizationë„ ê°€ëŠ¥í•œë°, \( \mathbf{\Sigma}^{-1} = \mathbf{L}^T \mathbf{L} \)ì„ ì •ì˜í•˜ì.
    <ul>
      <li>ì´ ë•Œ \( \mathbf{L}: \mathbb{R}^D \mapsto \mathbb{R}^d, \quad d \leq D \) ì´ë©°, mahalanobis distanceë¥¼ \( \mathbf{\delta} = \mathbf{y} - \mathbf{\mu} \)ë¥¼ ì´ìš©í•´ ì „ê°œí•´ë³´ë©´</li>
    </ul>
  </li>
</ul>

\[\Delta^2 = \| \mathbf{L} \mathbf{\delta} \|^2_2 \qquad{(3.92)}\]

<ul>
  <li>ìœ„ ì‹ì„ ì–»ëŠ”ë‹¤. ë”°ë¼ì„œ \( D \) ë³´ë‹¤ ì‘ì€ dimensionì¸ \( d \) dimensionì—ì„œì˜ Euclidean distanceë¡œë„ ìƒê°í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<h2 id="3-marginals-and-conditionals-of-an-mvn">3. Marginals and conditionals of an MVN</h2>
:ET