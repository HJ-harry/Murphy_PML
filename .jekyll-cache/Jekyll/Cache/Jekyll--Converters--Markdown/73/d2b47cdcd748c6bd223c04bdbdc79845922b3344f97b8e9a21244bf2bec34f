I"<ul>
  <li>ë³µì¡í•œ probiability modelì„ ë§Œë“œëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì€, ê°„ë‹¨í•œ probability distributionë“¤ì˜ convex combinationì„ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤.</li>
  <li>convex combinationì´ë€ ë¹„ìœ¨ì˜ í•©ì´ 1ì´ì–´ì•¼ í•œë‹¤ëŠ” ì˜ë¯¸ì´ê³ , ì‹ì„ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

\[p(\mathbf{y}|\boldsymbol{\theta}) = \sum_{k=1}^K \pi_k p_k(\mathbf{y}) \qquad{(3.112)}\]

<ul>
  <li>ìœ„ì™€ ê°™ì€ ëª¨ë¸ì„ <strong>hierarchical model</strong>ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ <strong>latent variable</strong> \( z \in {1, \dots, K} \)ë¥¼ ì´ìš©í•œë‹¤.
    <ul>
      <li>ì´ latent variableì€ output \( \mathbf{y} \)ë¥¼ ìœ„í•´ ì–´ë–¤ distributionì„ ì´ìš©í•  ì§€ë¥¼ ê²°ì •í•œë‹¤.</li>
      <li>ì´ latent variableì— ëŒ€í•œ priorëŠ” \( p(z = k) = \pi_k \)ì´ê³ ,</li>
      <li>conditional distributionì€ \( p(\mathbf{y}|z = k) = p_k(\mathbf{y}) = p(\mathbf{y}|\boldsymbol{\theta}_k)\)ë¡œ ì •í•´ì§„ë‹¤.</li>
      <li>ì •ë¦¬í•˜ìë©´ ì•„ë˜ì™€ ê°™ë‹¤.</li>
    </ul>
  </li>
</ul>

\[\begin{align}
p(z|\boldsymbol{\theta}) &amp;= \textrm{Cat}(z|\boldsymbol{\pi}) \qquad{(3.113)} \\
p(\mathbf{y}|z = k, \boldsymbol{\theta}) &amp;= p(\mathbf{y}|\boldsymbol{\theta}_k) \qquad{(3.114)}
\end{align}\]

<ul>
  <li>í’€ì–´ì„œ ìƒê°í•´ë³´ë©´, ìš°ì„  latent variableì¸\( z\)ë¥¼ categorical distributionìœ¼ë¡œë¶€í„° í•˜ë‚˜ ìƒ˜í”Œí•˜ê³ , ì´ì— conditioningëœ \( \mathbf{y} \)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì–»ëŠ” ì‹ìœ¼ë¡œ ì§„í–‰ëœë‹¤.</li>
  <li>ì—¬ê¸°ì„œ \( z \)ë¥¼ marginalize í•´ë³´ë©´</li>
</ul>

\[p(\mathbf{y}|\boldsymbol{\theta}) = \sum_{k=1}^K p(z=k|\boldsymbol{\theta})p(\mathbf{y}|z=k,\boldsymbol{\theta}) = \sum_{k=1}^K \pi_k p(\mathbf{y}|\boldsymbol{\theta}_k) \qquad{(3.115)}\]

<ul>
  <li>ê²°êµ­ ì‹ 3.112ì™€ ê°™ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
  <li>Mixture modelì˜ building blockì´ ë˜ëŠ” \( p() \) distributionì„ ë¬´ì—‡ìœ¼ë¡œ ê²°ì •í•˜ëƒì— ë”°ë¼ì„œ mixture modelë„ ë§ì´ ë‹¬ë¼ì§€ê²Œ ëœë‹¤. ê·¸ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì.</li>
</ul>

<h2 id="1-gaussian-mixture-models">1. Gaussian mixture models</h2>

<ul>
  <li>ê°€ì¥ ë§ì´ ì´ìš©ë˜ëŠ” mixture modelì¼ ê²ƒì´ë‹¤.
    <ul>
      <li><strong>Gausian mixture model (GMM)</strong> í˜¹ì€ <strong>mixture of Gaussians (MoG)</strong>ë¼ê³  ë¶ˆë¦°ë‹¤.</li>
    </ul>
  </li>
</ul>

\[p(\mathbf{y}) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{y}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \qquad{(3.116)}\]

<div class="text-center">
  <img src="/Murphy_PML/images/3.16.PNG" alt="Figure 3.16" height="300px" />
</div>
<ul>
  <li>ìœ„ ê·¸ë¦¼ì€ 3ê°œì˜ superpositionìœ¼ë¡œ êµ¬ì„±ëœ 2Dì—ì„œì˜ GMMì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.</li>
  <li>ì´ ê°¯ìˆ˜ë¥¼ ì¶©ë¶„íˆ í¬ê²Œ ë§Œë“¤ë©´, GMMìœ¼ë¡œ \( \mathbb{R}^D \)ì˜ ì–´ë– í•œ smooth distributionë„ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<h3 id="11-using-gmms-for-clustering">1.1 Using GMMs for clustering</h3>

<ul>
  <li>GMMì´ ë§ì´ ì“°ì´ëŠ” ë¶„ì•¼ëŠ” <strong>unsupervised clustering</strong>ì´ë‹¤.</li>
  <li>clusteringì´ ì§„í–‰ë˜ëŠ” ë°©ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.
    <ol>
      <li>MLE \( \hat{\boldsymbol{\theta}} = \text{argmax} \log p(\mathcal{D}|\boldsymbol{\theta}) \)ë¥¼ êµ¬í•œë‹¤.</li>
      <li>ê° data point \( \mathbf{y}_n \)ì— ê·¸ê²ƒì´ í•´ë‹¹í•˜ëŠ” latent variable \( z_n \in {1, \dots, K} \)ë¥¼ ë§¤ì¹­ì‹œí‚¨ë‹¤.</li>
    </ol>
  </li>
  <li>1ë²ˆì„ ì–´ë–»ê²Œ í•˜ëŠ”ì§€ëŠ” ë’¤ì— ë‚˜ì˜¬ ì„¹ì…˜ë“¤ì—ì„œ ì•Œì•„ë³´ì.</li>
  <li>2ë²ˆì€ ê²°êµ­ ê° data pointê°€ ì–´ëŠ clusterì— ì†í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë‚´ëŠ” ê³¼ì •ì¸ë°, ì´ë¥¼ <strong>responsitiblity</strong>ë¼ê³  í•˜ê³  posterior distributionì„ formalí•˜ê²Œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

\[r_{nk} := p(z_n = k|\mathbf{y}_n, \boldsymbol{\theta}) = \frac{p(z_n=k|\boldsymbol{\theta})p(\mathbf{x}_n|z_n = k, \boldsymbol{\theta})}{\sum_{k'=1}^K p(z_n = k'|\boldsymbol{\theta})p(\mathbf{y}_n|z_n = k', \boldsymbol{\theta})} \qquad{(3.51)}\]

<ul>
  <li>\( r_{nk} \)
    <ul>
      <li>\( n \): data point n</li>
      <li>\( k \): cluster k</li>
    </ul>
  </li>
  <li>ì—¬ê¸°ì„œ cluster \( k \)ì— ëŒ€í•œ MLEë¥¼ ê³„ì‚°í•˜ë©´</li>
</ul>

\[\hat{z}_n = \text{arg} \max_k r_{nk} = \text{arg} \max_k [\log p(\mathbf{y}_n|z_n = k, \boldsymbol{\theta}) + \log p(z_n = k|\boldsymbol{\theta})] \qquad{(3.118)}\]

<ul>
  <li>ìœ„ì™€ ê°™ì€ ë°©ì‹ì„ <strong>hard clustering</strong>ì´ë¼ê³  í•˜ê³ , ê° ë°ì´í„° í¬ì¸íŠ¸ì— í™•ë¥ ì ìœ¼ë¡œ ë‹¤ë¥¸ cluster classë¥¼ assigní•˜ëŠ” ë°©ì‹ì„ <strong>soft clustering</strong>ì´ë¼ê³  í•œë‹¤.</li>
  <li>\( z_n \)ì— uniform priorë¥¼ ë¶€ì—¬í•˜ì—¬ ëª¨ë“  clusterì— ê°™ì€ í™•ë¥ ì„ ë¶€ì—¬í•˜ê³ , \( k \)ë²ˆì§¸ Gaussianì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°ê° \( \hat{\boldsymbol{\mu}_k}, \boldsymbol{\Sigma}_k = \mathbf{I} \)ë¼ê³  í•˜ë©´, ê²°êµ­ ê° clusterì˜ í‰ê· ì ê³¼ì˜ mseë¥¼ minimizeí•˜ëŠ” ë¬¸ì œê°€ ëœë‹¤.</li>
</ul>

\[z_n = \text{arg}\min_k \|\mathbf{y}_n - \hat{\boldsymbol{\mu}}_k\|_2^2\]

<ul>
  <li>ì´ëŠ” <strong>K-means clustering algorithm</strong>ì˜ ê¸°ë³¸ì´ ëœë‹¤.</li>
</ul>

<h3 id="12-using-gmms-as-a-prior-to-regularize-an-inverse-problem">1.2 Using GMMs as a prior to regularize an inverse problem</h3>

<p>\(\mathbf{y} = \mathbf{W}\mathbf{z} + \sigma^2 \mathbf{I}\)</p>
<ul>
  <li>ìœ„ì™€ ê°™ì€ í”í•œ inverse problemì„ ìƒê°í•´ë³´ì.
    <ul>
      <li>ì´ ë•Œ noiseê°€ white gaussianì´ë¼ëŠ” assumptionì´ ë“¤ì–´ê°”ëŠ”ë°, ì´ëŠ” í”íˆ ì´ìš©ë˜ëŠ” ê°€ì •ì´ë‹¤.</li>
      <li>\( \mathbf{W} \)ëŠ” ì¼ë°˜ì ìœ¼ë¡œ forward opeartorë¼ê³  í•˜ë©°, denoisingì¼ ì‹œ identity matrix, deblurring(deconvolution)ì¼ ì‹œ íŠ¹ì • ì‚¬ì´ì¦ˆì˜ blur kernel ë“±ì´ ëœë‹¤.</li>
    </ul>
  </li>
  <li>ì´ë¥¼ likelihoodë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</li>
</ul>

\[p(\mathbf{y}|\mathbf{z}) = \mathcal{N}(\mathbf{y}|\mathbf{W}\mathbf{z}, \sigma^2 \mathbf{I}) \qquad{(3.120)}\]

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>ê²°êµ­ inverse problemì„ êµ¬í•˜ëŠ” ë¬¸ì œëŠ” MAP estimate \( \hat{\mathbf{z}} = \text{argmax}p(\mathbf{z}</td>
          <td>\mathbf{y}) \)ì„ êµ¬í•˜ëŠ” ë¬¸ì œë¡œë„ í‘œí˜„ì´ ë  ìˆ˜ ìˆë‹¤.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>ë‹¤ë§Œ, inverse problemì˜ íŠ¹ì„± ìƒ ë¬´ìˆ˜íˆ ë§ì€ (ì„œë¡œ ë‹¤ë¥¸) \( \mathbf{z} \)ê°€ ê°™ì€ \( \mathbf{y} \)ë¡œ ë§¤í•‘ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— solution spaceë¥¼ ì œí•œí•˜ê¸° ìœ„í•´ <strong>regularization</strong>ì´ í•„ìˆ˜ì ì´ë‹¤.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>\( \mathbf{z} \)ì— ëŒ€í•œ priorë¡œ GMMì„ ì‚¬ìš©í•œë‹¤ë©´, \( p(\mathbf(z)) = \sum_{k=1}^K p(c=k)\mathcal{N}(\mathbf{z}</td>
          <td>\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \)ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê³ , joint distributionì€ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

\[\begin{align}
p(\mathbf{y}, \mathbf{z}) &amp;= p(\mathbf{z})p(\mathbf{y}|\mathbf{z})\\
&amp;= p(c=k)\mathcal{N}(\mathbf{z}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\mathcal{N}(\mathbf{y}|\mathbf{W}\mathbf{z}, \sigma^2 \mathbf{I})\]
:ET