I"Ş<ul>
  <li>ì§€ë‚œ ë‹¨ì›ì— ì´ì–´ì„œ labelì´ binaryê°€ ì•„ë‹Œ finite setì¼ ë•Œë¥¼ ë‹¤ë£¬ë‹¤. ë‹¤ì‹œ ë§í•´, \( y \in {1, \dots, C} \) ì¸ ê²½ìš°ì´ë‹¤.</li>
</ul>

<h3 id="1-definition">1. Definition</h3>

\[\textrm{Cat}(y|\mathbf{\mu}) := \prod_{c=1}^C \theta_{c}^{\mathbb{I}(y=c)} \qquad{(3.22)}\]

<ul>
  <li>ë‹¤ì‹œ ë§í•´, \( y \) ê°€ \( c \) ë¼ëŠ” ê°’ì„ ê°€ì§ˆ ë•Œì˜ í™•ë¥ ì„ ê°ê° \( \theta_c \) ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</li>
  <li>ë‹¹ì—°í•˜ê²Œë„, ì•„ë˜ ë‘ ì¡°ê±´ì„ ì¶©ì¡±ì‹œì¼œì•¼ í•œë‹¤.
    <ul>
      <li>\( 0 \leq \theta_c \leq 1 \)</li>
      <li>\( \Sigma_{c=1}^C \theta_c = 1 \)</li>
    </ul>
  </li>
  <li>practical í•˜ê²ŒëŠ” discrete variableì¸ \( y \) ë¥¼ <strong>one-hot encoding</strong> ì‹œí‚¤ëŠ” ê²½ìš°ê°€ ëŒ€ë¶€ë¶„ì´ë‹¤.</li>
  <li>ì´ ê²½ìš° ê°ê°ì˜ classëŠ” \( (1, 0, 0), (0, 1, 0), (0, 0, 1) \) ë“±ì˜ <strong>unit vector</strong>ë¡œ encoding ë˜ë©°, í•´ë‹¹í•˜ëŠ” class \( c \) ë¥¼ ì œì™¸í•˜ê³ ëŠ” 0ì¸ ê°’ì„ ê°–ê²Œ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ encodingì´ ëœë‹¤.</li>
  <li>ë§Œì•½ \( \mathbf{y} \) ë¥¼ one-hot vectorë¡œ ê³ ë ¤í•œë‹¤ë©´ categorical distributionì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</li>
</ul>

\[\textrm{Cat}(\mathbf{y}|\mathbf{\mu}) := \prod_{c=1}^C \theta_{c}^{y_c} \qquad{(3.23)}\]

<ul>
  <li>bionomial distributionì´ bernoulli distributionì˜ generalizationì´ì—ˆë˜ ê²ƒì²˜ëŸ¼, categorical distributionì˜ generalizationì¸ <strong>multinomial distribution</strong>ë„ ì¡´ì¬í•œë‹¤.</li>
  <li>\( N \) ë²ˆì˜ categorical trialì´ ìˆë‹¤ê³  ìƒê°í•˜ë©´, multinomial distributionì€ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</li>
</ul>

\[\textrm{Mu}(\mathbf{s}|N, \mathbf{\mu}) := \begin{pmatrix} N \\ s_{1} \dots s_{C} \end{pmatrix} \prod_{c=1}^C \theta_{c}^{s_c} \qquad{(3.24)}\]

<h3 id="2-softmax-function">2. Softmax function</h3>

<ul>
  <li>conditional discriminative modelì„ ìƒê°í–ˆì„ ë•Œ, output distributionì„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

\[p(y|\mathbf{x},\mathbf{\theta}) = \textrm{Mu}(\mathbf{y}|1, f(\mathbf{x}; \mathbf{\theta})) \qquad{(3.27)}\]

<ul>
  <li>ê° classì˜ í™•ë¥ ì˜ í•©ì´ 1ì´ì–´ì•¼ í•˜ë©°, ê° componentê°€ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§„ë‹¤ëŠ” ì¡°ê±´ì„ ì¶©ì¡±ì‹œì¼œì•¼ í•œë‹¤.</li>
  <li>binary caseì—ì„œëŠ” ì´ë¥¼ ìœ„í•´ í•¨ìˆ˜ \( f \) ë¥¼ sigmoid í•¨ìˆ˜ë¥¼ ì´ìš©í•´ squeeze í–ˆì—ˆë‹¤.</li>
  <li>ê·¸ì— ëŒ€í•œ multi-classì˜ counterpartë¥¼ <strong>softmax function</strong>, ë˜ëŠ” <strong>multinomial logit</strong> ì´ë¼ê³  í•œë‹¤.</li>
</ul>

\[\mathcal{S}(\mathbf{a}) := [\frac{e^{a_1}}{\Sigma_{c'=1}^C e^{a_{c'}}}, \dots , \frac{e^{a_C}}{\Sigma_{c'=1}^C e^{a_{c'}}}] \qquad{(3.28)}\]

<ul>
  <li>ìœ„ í•¨ìˆ˜ëŠ” \( \mathbb{R}^C \) ë¥¼ \( [0, 1]^C \) ë¡œ squeezeí•´ì¤€ë‹¤.</li>
  <li>eq. (3.28) ì— ëŒ€í•´ inputìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” \( \mathbf{a} = f(\mathbf{x}; \mathbf{\theta}) \) ëŠ” <strong>logit</strong>ì´ë¼ê³  ë¶€ë¥´ë©°, binary caseì˜ log odds ì˜ generalizationì´ë‹¤.</li>
  <li>sigmoid functionì´ heaviside step functionì„ ê·¼ì‚¬í•˜ëŠ” ë°ì— ì´ìš©ë  ìˆ˜ ìˆì—ˆë˜ ê²ƒì²˜ëŸ¼, softmax functionì— temperatureë¥¼ ì´ìš©í•˜ì—¬ <strong>argmax function</strong> ì²˜ëŸ¼ behaveí•˜ê²Œ í•  ìˆ˜ ìˆë‹¤.</li>
  <li>softmax functionì˜ temperatureëŠ” \( \mathcal{S}(\mathbf{a}/T) \) ì˜ \( T \) ì´ë©°, ì´ ê°’ì´ 0ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ argmax functionì— ê·¼ì‚¬í•œë‹¤.</li>
  <li>ë°˜ëŒ€ë¡œ, \( T \) ì˜ ê°’ì´ ì»¤ì§€ë©´ ì»¤ì§ˆìˆ˜ë¡ outputì´ uniformí•´ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤.</li>
</ul>

<h3 id="3-multiclass-logistic-regression">3. Multiclass logistic regression</h3>

<ul>
  <li>í•¨ìˆ˜ \( f(\dot) \) ë¡œ linear predictorë¥¼ ì´ìš©í•˜ë©´ ìµœì¢… ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ë‹¤.</li>
</ul>

\[p(y|\mathbf{x};\mathbf{\theta}) = \textrm{Cat}(y|\mathcal{S}(\mathbf{W}\mathbf{x} + \mathbf{b})) \qquad{(3.30)}\]

<ul>
  <li>ì—¬ê¸°ì„œ ì–»ì–´ì§€ëŠ” \( \mathbf{a} = \mathbf{W}\mathbf{x} + \mathbf{b}\) ëŠ” <strong>logit</strong> ê°’ì´ ë˜ë©°, ì´ë¥¼ softmax functionì„ í†µê³¼ì‹œì¼œ probabilityë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.</li>
  <li>ì´ë¥¼ <strong>multinomial logistic regression</strong> ì´ë¼ê³  ì¹­í•œë‹¤.</li>
</ul>

<h3 id="4-log-sum-exp-trick">4. Log-sum-exp trick</h3>

<ul>
  <li>logitì„ softmax functionì— í†µê³¼ì‹œì¼œ í™•ë¥  ê°’ì„ ì–»ëŠ” ë°ì— numerical problemì´ ìˆì„ ìˆ˜ ìˆë‹¤.</li>
</ul>

\[p_c = \frac{e^{a_c}}{Z(\mathbf{a})} = \frac{e^{a_c}}{\Sigma_{c'=1}^C e^{a_{c'}}} \qquad{(3.33)}\]

<ul>
  <li>ë¶„ëª¨ì— ìˆëŠ” <strong>partition function</strong> \( Z \)ë¥¼ ê³„ì‚°í•  ë•Œ, ë”í•´ì§€ëŠ” ê° í•­ì´ <code class="language-plaintext highlighter-rouge">exp</code> í•¨ìˆ˜ì´ê¸°ì— logit ê°’ì´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ overflow/underflow ë“±ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">np.exp(1000) = inf</code></li>
      <li><code class="language-plaintext highlighter-rouge">np.exp(-1000) = 0</code></li>
    </ul>
  </li>
  <li>ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì•„ë˜ propertyë¥¼ ì´ìš©í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

\[\log \sum_{c=1}^C \exp (a_c) = m + \log \sum_{c=1}^C \exp (a_c - m) \qquad{(3.34)}\]

<ul>
  <li>ìœ„ propertyëŠ” arbitraryí•œ \( m \) ì— ëª¨ë‘ ì ìš©ë˜ë©°, ë”°ë¼ì„œ overflowë¥¼ ë§‰ê¸° ìœ„í—¤ ë³´í†µ \(  m = \max_c a_c \) ë¥¼ ì´ìš©í•œë‹¤.</li>
  <li>ì´ëŸ¬í•œ propertyë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì„ <strong>log-sum-exp trick</strong> ì´ë¼ê³  ë¶€ë¥´ëŠ”ë°, <code class="language-plaintext highlighter-rouge">lse</code> í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë°ì— ì“°ì´ê¸° ë•Œë¬¸ì´ë‹¤.</li>
</ul>

\[\textrm{lse}(\mathbf{a}) := \log \sum_{c=1}^C \exp (a_c) \qquad{(3.35)}\]

<ul>
  <li>ì´ë¥¼ ì´ìš©í•˜ë©´ logitì„ ì´ìš©í•´ í™•ë¥ ê°’ë“¤ì„ êµ¬í•  ìˆ˜ ìˆëŠ”ë°, ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</li>
</ul>

\[\begin{align}
  p_c &amp;= \exp (a_c - \log \sum_{c'=1}^C e^{a_{c'}}) \\
  &amp;= \exp (a_c - \textrm{lse}(\mathbf{a}))
\end{align}\]

<ul>
  <li>ì´í›„ì— ì´ë¥¼ ì´ìš©í•´ <code class="language-plaintext highlighter-rouge">cross-entropy</code> lossë¥¼ ê³„ì‚° í•˜ëŠ” ë°ì— ì´ìš©í•  ìˆ˜ ìˆë‹¤.</li>
  <li>ë¬¼ë¡  ì´ëŸ° trick ì—†ì´ logitìœ¼ë¡œë¶€í„° ì§ì ‘ì ìœ¼ë¡œ <code class="language-plaintext highlighter-rouge">CE loss</code>ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤.</li>
</ul>

\[\mathcal{L} = - [\mathbb{I}(y = 0)\log p_0 + \mathbb{I}(y = 1) \log p_1] \qquad{(3.37)}\]

<ul>
  <li>ìœ„ì™€ ê°™ì´ ì •ì˜ëœ <code class="language-plaintext highlighter-rouge">BCE loss</code>ë¥¼ ê³„ì‚°í•  ë•Œ, ê° í•­ì€ ì•„ë˜ì™€ ê°™ì´ \( lse() \) í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

\[\log p_1 = \log (\frac{1}{1 + \exp (-a)}) = \log (1) - log (1 + \exp (-a)) = 0 - \textrm{lse}([0, -a]) \qquad{(3.38)}\]

\[\log p_0 = \log (\frac{1}{1 + \exp (+a)}) = \log (1) - log (1 + \exp (+a)) = 0 - \textrm{lse}([0, +a]) \qquad{(3.39)}\]

:ET