I"j<ul>
  <li>복잡한 probiability model을 만드는 가장 단순한 방법은, 간단한 probability distribution들의 convex combination을 이용하는 것이다.</li>
  <li>convex combination이란 비율의 합이 1이어야 한다는 의미이고, 식을 아래와 같이 표현할 수 있다.</li>
</ul>

\[p(\mathbf{y}|\boldsymbol{\theta}) = \sum_{k=1}^K \pi_k p_k(\mathbf{y}) \qquad{(3.112)}\]

<ul>
  <li>위와 같은 모델을 <strong>hierarchical model</strong>로 표현하기 위해 새로운 <strong>latent variable</strong> \( z \in {1, \dots, K} \)를 이용한다.
    <ul>
      <li>이 latent variable은 output \( \mathbf{y} \)를 위해 어떤 distribution을 이용할 지를 결정한다.</li>
      <li>이 latent variable에 대한 prior는 \( p(z = k) = \pi_k \)이고,</li>
      <li>conditional distribution은 \( p(\mathbf{y}|z = k) = p_k(\mathbf{y}) = p(\mathbf{y}|\boldsymbol{\theta}_k)\)로 정해진다.</li>
      <li>정리하자면 아래와 같다.</li>
    </ul>
  </li>
</ul>

\[\begin{align}
p(z|\boldsymbol{\theta}) &amp;= \textrm{Cat}(z|\boldsymbol{\pi}) \qquad{(3.113)} \\
p(\mathbf{y}|z = k, \boldsymbol{\theta}) &amp;= p(\mathbf{y}|\boldsymbol{\theta}_k) \qquad{(3.114)}\]

<ul>
  <li>풀어서 생각해보면, 우선 latent variable인\( z\)를 categorical distribution으로부터 하나 샘플하고, 이에 conditioning된 \( \mathbf{y} \)를 순차적으로 얻는 식으로 진행된다.</li>
  <li>여기서 \( z \)를 marginalize 해보면</li>
</ul>

\[p(\mathbf{y}|\boldsymbol{\theta}) = \sum_{k=1}^K p(z=k|\boldsymbol{\theta})p(\mathbf{y}|z=k,\boldsymbol{\theta}) = \sum_{k=1}^K \pi_k p(\mathbf{y}|\boldsymbol{\theta}_k) \qquad{(3.115)}\]

<ul>
  <li>결국 식 3.112와 같은 것을 알 수 있다.</li>
  <li>Mixture model의 building block이 되는 \( p() \) distribution을 무엇으로 결정하냐에 따라서 mixture model도 많이 달라지게 된다. 그 예시를 살펴보자.</li>
</ul>

<h2 id="1-gaussian-mixture-models">1. Gaussian mixture models</h2>

<ul>
  <li>가장 많이 이용되는 mixture model일 것이다.
    <ul>
      <li><strong>Gausian mixture model (GMM)</strong> 혹은 <strong>mixture of Gaussians (MoG)</strong>라고 불린다.</li>
    </ul>
  </li>
</ul>

\[p(\mathbf{y}) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{\y}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \qquad{(3.116)}\]

<div class="text-center">
  <img src="/Murphy_PML/images/3.16.PNG" alt="Figure 3.16" height="300px" />
</div>
<ul>
  <li>위 그림은 3개의 superposition으로 구성된 2D에서의 GMM을 보여주고 있다.</li>
  <li>이 갯수를 충분히 크게 만들면, GMM으로 \( \mathbb{R}^D \)의 어떠한 smooth distribution도 근사할 수 있다.</li>
</ul>

<h3 id="11-using-gmms-for-clustering">1.1 Using GMMs for clustering</h3>

<ul>
  <li>GMM이 많이 쓰이는 분야는 <strong>unsupervised clustering</strong>이다.</li>
  <li>clustering이 진행되는 방식은 아래와 같다.
    <ol>
      <li>
        <table>
          <tbody>
            <tr>
              <td>MLE \( \hat{\boldsymbol{\theta}} = \text{argmax} \log p(\mathcal{D}</td>
              <td>\boldsymbol{\theta}) \)를 구한다.</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>각 data point \( \mathbf{y}_n \)에 그것이 해당하는 latent variable \( z_n \in {1, \dots, K} \)를 매칭시킨다.</li>
    </ol>
  </li>
</ul>
:ET