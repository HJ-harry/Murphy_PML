I"<ul>
  <li>ë³µì¡í•œ probiability modelì„ ë§Œë“œëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì€, ê°„ë‹¨í•œ probability distributionë“¤ì˜ convex combinationì„ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤.</li>
  <li>convex combinationì´ë€ ë¹„ìœ¨ì˜ í•©ì´ 1ì´ì–´ì•¼ í•œë‹¤ëŠ” ì˜ë¯¸ì´ê³ , ì‹ì„ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

\[p(\mathbf{y}|\boldsymbol{\theta}) = \sum_{k=1}^K \pi_k p_k(\mathbf{y}) \qquad{(3.112)}\]

<ul>
  <li>ìœ„ì™€ ê°™ì€ ëª¨ë¸ì„ <strong>hierarchical model</strong>ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ <strong>latent variable</strong> \( z \in {1, \dots, K} \)ë¥¼ ì´ìš©í•œë‹¤.
    <ul>
      <li>ì´ latent variableì€ output \( \mathbf{y} \)ë¥¼ ìœ„í•´ ì–´ë–¤ distributionì„ ì´ìš©í•  ì§€ë¥¼ ê²°ì •í•œë‹¤.</li>
      <li>ì´ latent variableì— ëŒ€í•œ priorëŠ” \( p(z = k) = \pi_k \)ì´ê³ ,</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>conditional distributionì€ \( p(\mathbf{y}</td>
              <td>z = k) = p_k(\mathbf{y}) = p(\mathbf{y}</td>
              <td>\boldsymbol{\theta}_k)\)ë¡œ ì •í•´ì§„ë‹¤.</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>ì •ë¦¬í•˜ìë©´ ì•„ë˜ì™€ ê°™ë‹¤.</li>
    </ul>
  </li>
</ul>

\[\begin{align}
p(z|\boldsymbol{\theta}) &amp;= \text{Cat}(z|\boldsymbol{\pi}) \qquad{(3.113)}
p(\mathbf{y}|z = k, \boldsymbol{\theta}) &amp;= p(\mathbf{y}|\boldsymbol{\theta}_k) \qquad{(3.114)}\]

<ul>
  <li>í’€ì–´ì„œ ìƒê°í•´ë³´ë©´, ìš°ì„  latent variableì¸\( z\)ë¥¼ categorical distributionìœ¼ë¡œë¶€í„° í•˜ë‚˜ ìƒ˜í”Œí•˜ê³ , ì´ì— conditioningëœ \( \mathbf{y} \)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì–»ëŠ” ì‹ìœ¼ë¡œ ì§„í–‰ëœë‹¤.</li>
  <li>ì—¬ê¸°ì„œ \( z \)ë¥¼ marginalize í•´ë³´ë©´</li>
</ul>

\[p(\mathbf{y}|\boldsymbol{\theta}) = \sum_{k=1}^K p(z=k|\boldsymbol{\theta})p(\mathbf{y}|z=k,\boldsymbol{\theta}) = \sum_{k=1}^K \pi_k p(\mathbf{y}|\boldsymbol{\theta}_k) \qquad{(3.115)}\]

<ul>
  <li>ê²°êµ­ ì‹ 3.112ì™€ ê°™ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
  <li>Mixture modelì˜ building blockì´ ë˜ëŠ” \( p() \) distributionì„ ë¬´ì—‡ìœ¼ë¡œ ê²°ì •í•˜ëƒì— ë”°ë¼ì„œ mixture modelë„ ë§ì´ ë‹¬ë¼ì§€ê²Œ ëœë‹¤. ê·¸ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì.</li>
</ul>

<h2 id="1-gaussian-mixture-models">1. Gaussian mixture models</h2>

<ul>
  <li>ê°€ì¥ ë§ì´ ì´ìš©ë˜ëŠ” mixture modelì¼ ê²ƒì´ë‹¤.
    <ul>
      <li><strong>Gausian mixture model (GMM)</strong> í˜¹ì€ <strong>mixture of Gaussians (MoG)</strong>ë¼ê³  ë¶ˆë¦°ë‹¤.</li>
    </ul>
  </li>
</ul>

\[p(\mathbf{y}) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{\y}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \qquad{(3.116)}\]

<div class="text-center">
  <img src="/Murphy_PML/images/3.16.PNG" alt="Figure 3.16" height="300px" />
</div>
<ul>
  <li>ìœ„ ê·¸ë¦¼ì€ 3ê°œì˜ superpositionìœ¼ë¡œ êµ¬ì„±ëœ 2Dì—ì„œì˜ GMMì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.</li>
  <li>ì´ ê°¯ìˆ˜ë¥¼ ì¶©ë¶„íˆ í¬ê²Œ ë§Œë“¤ë©´, GMMìœ¼ë¡œ \( \mathbb{R}^D \)ì˜ ì–´ë– í•œ smooth distributionë„ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<h3 id="11-using-gmms-for-clustering">1.1 Using GMMs for clustering</h3>

<ul>
  <li>GMMì´ ë§ì´ ì“°ì´ëŠ” ë¶„ì•¼ëŠ” <strong>unsupervised clustering</strong>ì´ë‹¤.</li>
  <li>clusteringì´ ì§„í–‰ë˜ëŠ” ë°©ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.
    <ol>
      <li>
        <table>
          <tbody>
            <tr>
              <td>MLE \( \hat{\boldsymbol{\theta}} = \text{argmax} \log p(\mathcal{D}</td>
              <td>\boldsymbol{\theta}) \)ë¥¼ êµ¬í•œë‹¤.</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>ê° data point \( \mathbf{y}_n \)ì— ê·¸ê²ƒì´ í•´ë‹¹í•˜ëŠ” latent variable \( z_n \in {1, \dots, K} \)ë¥¼ ë§¤ì¹­ì‹œí‚¨ë‹¤.</li>
    </ol>
  </li>
</ul>
:ET