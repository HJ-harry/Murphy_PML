I"×!<ul>
  <li>ë°”ë¡œ ì „ ì„¹ì…˜ì—ì„œ Gaussian random vectorì˜ ë¹ˆ ë¶€ë¶„ì„ ì±„ìš°ëŠ” ë°©ë²•ì„ ê³µë¶€í–ˆë‹¤.</li>
  <li>ì´ì œ ê·¸ê²ƒì„ ë„˜ì–´, <strong>noisy observation</strong>ì´ ìˆëŠ” ê²½ìš° ì–´ë–»ê²Œ í•´ì•¼ í•˜ëŠ” ì§€ë¥¼ ì•Œì•„ë³´ì.</li>
  <li>ìš°ì„ , \( \mathbf{z} \in \mathbb{R}^D \)ê°€ unknown vector valueì´ê³ , \( \mathbf{y} \in \mathbb{R}^K \) ê°€ noisy observationì´ë¼ê³  ê°€ì •í•˜ì.</li>
</ul>

\[\begin{align}
p(\mathbf{z}) &amp;= \mathcal{N}(\boldsymbol{\mu}_z, \mathbf{\Sigma}_z) \qquad{(3.98)}\\
p(\mathbf{y}|\mathbf{z}) &amp;= \mathcal{N}(\mathbf{y}|\mathbf{W}\mathbf{z} + \mathbf{b}, \mathbf{\Sigma}_y) \qquad{(3.99)}
\end{align}\]

<ul>
  <li>ìœ„ì™€ ê°™ì´ \( \mathbf{y} \)ëŠ” \( \mathbf{z} \) ì— dependentí•˜ë©°, \( \mathbf{W} \in \mathbb{R}^{D \times K}\)matrixë¥¼ í†µí•´ ê´€ê³„ì‹ì´ ì •ë¦½ëœë‹¤.</li>
  <li>\( p(\mathbf{z}, \mathbf{y}) = p(\mathbf{z})p(\mathbf{y}|\mathbf{z}) \)ì˜ joint distributionì˜ í‰ê· ê³¼ ë¶„ì‚°ì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ë˜ë©°, ì¦ëª…ì€ ë”°ë¡œ ì •ë¦¬í•´ë‘ì—ˆë‹¤. <a href="/Murphy_PML/solutions/LGS.pdf">proof</a></li>
</ul>

\[\begin{align}
\boldsymbol{\mu} &amp;= \begin{pmatrix} \boldsymbol{\mu}_z \\ \mathbf{W}\boldsymbol{\mu} + \mathbf{b} \end{pmatrix} \qquad{(3.100)}\\
\boldsymbol{\Sigma} &amp;= \begin{pmatrix} \boldsymbol{\Sigma}_z &amp; \boldsymbol{\Sigma}_z \mathbf{W}^\top \\ \mathbf{W}\boldsymbol{\Sigma}_z &amp; \boldsymbol{\Sigma}_x + \mathbf{W}\boldsymbol{\Sigma}_z \mathbf{W}^\top \end{pmatrix} \qquad{(3.101)}
\end{align}\]

<ul>
  <li>ë§ˆì°¬ê°€ì§€ë¡œ, Bayes ruleì„ ì´ìš©í•´ì„œ posterior \( p(\mathbf{z}|\mathbf{y}) \)ì— ëŒ€í•œ parameter ê°’ë„ analyticí•˜ê²Œ êµ¬í•  ìˆ˜ ìˆìœ¼ë©°, ìœ„ ì¦ëª…ì— ê°™ì´ ì •ë¦¬ë˜ì–´ìˆë‹¤.
    <ul>
      <li>ì´ë¥¼ <strong>Bayesâ€™ rule for Gaussians</strong> ë¼ê³  í•œë‹¤.</li>
    </ul>
  </li>
</ul>

\[\begin{align}
p(\mathbf{z}|\mathbf{y}) &amp;= \frac{p(\mathbf{y}|\mathbf{z})p(\mathbf{z})}{p(\mathbf{y})} = \mathcal{N}(\mathbf{z}|\boldsymbol{\mu}_{z|y},\boldsymbol{\Sigma}_{z|y}) \qquad{(3.102)}\\
\boldsymbol{\Sigma}_{z|y}^{-1} &amp;= \boldsymbol{\Sigma}_z^{-1} + \mathbf{W}^\top \boldsymbol{\Sigma}_y^{-1}\mathbf{W} \qquad{(3.103)}\\
\boldsymbol{\mu}_{z|y} &amp;= \boldsymbol{\Sigma}_{z|y}[\mathbf{W}^\top \boldsymbol{\Sigma}_y^{-1}(\mathbf{y} - \mathbf{b}) + \boldsymbol{\Sigma}_z^{-1} \boldsymbol{\mu}_z] \qquad{(3.104)}
\end{align}\]

<ul>
  <li>ë§ˆì°¬ê°€ì§€ë¡œ, normalization constant \( p(\mathbf{y}) \)ë„ Gaussianì˜ í˜•íƒœë¡œ ì •ë¦¬ê°€ ë˜ë©°, ë”°ë¼ì„œ <strong>prior, posterior, marginal ëª¨ë‘ Gaussian</strong>ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
    <blockquote>
      <p>Gaussian prior ê³¼ Gaussian likelihoodê°€ í•©í•´ì§€ë©´ Gaussian posteriorë¥¼ ì–»ì„ ìˆ˜ ìˆê³ , ì´ì™€ ê°™ì´ ê°™ì€ í˜•íƒœì˜ likelihood/posteriorë¥¼ ì–»ì„ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” likelihoodë¥¼ <strong>conjugate prior</strong>ë¼ê³  í•œë‹¤.</p>
    </blockquote>
  </li>
</ul>

<h2 id="1-example-inferring-a-latent-vector-from-a-noisy-sensor">1. Example: inferring a latent vector from a noisy sensor</h2>

<ul>
  <li>ìš°ë¦¬ê°€ ì•Œê³ ì í•˜ëŠ” ë²¡í„° \( \mathbf{z} \in \mathbb{R}^D \)ê°€ ìˆë‹¤ê³  í•˜ì.
    <ul>
      <li>ì´ì— ëŒ€í•œ priorëŠ” Gaussian prior \( p(\mathbf{z}) = \mathcal{N}(\boldsymbol{\mu}_z, \boldsymbol{\Sigma}_z) \)ë¡œ ì£¼ì–´ì§„ë‹¤.</li>
      <li>ë§Œì•½ ì•„ëŠ”ê²Œ ì•„ë¬´ê²ƒë„ ì—†ë‹¤ë©´, \( \boldsymbol{\Sigma}_z = \infty \mathbf{I} \)ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ í•©ë¦¬ì ì¼ ê²ƒì´ë‹¤.</li>
      <li>ì•„ëŠ”ê²Œ ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë¯€ë¡œ, í‰ê·  ë˜í•œ 0ìœ¼ë¡œ ë‘ì.</li>
    </ul>
  </li>
  <li>\( \mathbf{z} \)ì— ëŒ€í•œ noisy observation \( \mathbf{y}_n \)ì„ Në²ˆ ìƒ˜í”Œë§ í–ˆë‹¤ê³  ìƒê°í•˜ì. (dimension KëŠ” ì˜¤íƒ€ì¸ë“¯..?)
\(p(\mathcal{D}|\mathbf{z}) = \prod_{n=1}^{N} \mathcal{N}(\mathbf{y}_n|\mathbf{z}, \boldsymbol{\Sigma}_y) = \mathcal{N}(\bar{\mathbf{y}}|\mathbf{z}, \frac{1}{N}\boldsymbol{\Sigma}_y) \qquad{(3.40)}\)</li>
  <li>ìœ„ ì‹ì„ ì •ë¦¬í•  ë•Œ \( N \) ê°œì˜ observation í‰ê·  \( \bar{\mathbf{y}} \)ì„ ì´ìš©í•˜ì—¬ ëŒ€ì²´í–ˆê³ , ê·¸ì— ë§ê²Œ covariance matrixë¥¼ \( N \)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì£¼ëŠ” trickì„ ì´ìš©í–ˆë‹¤.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>ì—¬ê¸°ì„œ \( \mathbf{W} = \mathbf{I}, \mathbf{b} = \mathbf{0} \)ë¥¼ ì ìš©í•˜ë©´, ìœ„ì—ì„œ ì •ë¦¬í•œ Bayes rule for Gaussian ì‹ì„ ê·¸ëŒ€ë¡œ ì ìš©í•  ìˆ˜ ìˆê³ , posterior distribution \( p(\mathbf{z}</td>
          <td>\mathcal{D}) \)ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>
<div class="text-center">
  <img src="/Murphy_PML/images/3.14.PNG" alt="Figure 3.14" height="300px" />
</div>
<ul>
  <li>ìœ„ ê·¸ë¦¼ì€ ì´ëŸ¬í•œ linear Gaussian systemì˜ 2D ì˜ˆì œë¥¼ ë³´ì—¬ì¤€ë‹¤.
    <ul>
      <li>\( \mathbf{z} \): 2d ê³µê°„ì˜ ì¢Œí‘œ</li>
      <li>\( \mathbf{y}_n \): ì¢Œí‘œì˜ noisy observation</li>
    </ul>
  </li>
  <li>\( \mathbf{y}_n \)ì„ ë” ë§ì´ ê´€ì¸¡í• ìˆ˜ë¡, ì‹¤ì œ ê°’ \( \mathbf{z} \)ì„ ë” ì‰½ê²Œ ìœ ì¶”í•  ìˆ˜ ìˆê²Œ ëœë‹¤.
    <ul>
      <li>ì´ë¥¼ temporal axisë¡œê¹Œì§€ ì—°ì¥í•œ ì•Œê³ ë¦¬ì¦˜ì„ <strong>Kalman filter</strong> ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³  í•œë‹¤.</li>
    </ul>
  </li>
  <li>ìœ„ ê·¸ë¦¼ì˜ ë§¨ ì˜¤ë¥¸ìª½ì„ ë³´ë©´ posterior distributionì´ ì–´ë–»ê²Œ í˜•ì„±ë˜ì—ˆëŠ”ì§€ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. \( \mathbf{z} \)ì˜ ê° ì¢Œí‘œ ì¤‘ ë” ë§ì´ í¼ì§„ horizontal axisê°€ uncertaintyê°€ ë” ë†’ê³ , ë”°ë¼ì„œ í•´ë‹¹ sensorì— ëŒ€í•œ reliabilityê°€ ë–¨ì–´ì§„ë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<h2 id="2-example-inferring-a-latent-vector-from-multiple-noisy-sensors">2. Example: inferring a latent vector from multiple noisy sensors</h2>

<ul>
  <li>ì´ì œ ë°”ë¡œ ì•ì„  ì„¹ì…˜ì—ì„œ í’€ì—ˆë˜ noisy sensorë¬¸ì œë¥¼ <strong>sensorê°€ ì—¬ëŸ¬ê°œ</strong> ì¸ ìƒí™©ì—ì„œ ê°€ì •í•´ë³´ì.</li>
</ul>

\[p(\mathbf{z}, \mathbf{y}) = p(\mathbf{z})\prod_{m=1}^M \prod_{n=1}^{N_m} \mathcal{N}(\mathbf{y}_{n,m}|\mathbf{z},\boldsymbol{\Sigma}_m) \qquad{(3.44)}\]

<ul>
  <li>íŒŒë¼ë¯¸í„°ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.
    <ul>
      <li>\( M \): ì„¼ì„œì˜ ê°¯ìˆ˜</li>
      <li>\( N_m \): ì„¼ì„œ \( m \)ì—ì„œ observationì˜ ê°¯ìˆ˜</li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>ë”°ë¼ì„œ, Mê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì„¼ì„œì—ì„œ ì–»ì€ Nê°œì˜ observationë“¤ë¡œ posterior \( p(\mathbf{z}</td>
          <td>Â </td>
          <td>\mathbf{y}) \)ë¥¼ ì¶”ë¡ í•´ë‚´ëŠ” ê²ƒì´ ìš°ë¦¬ì˜ ëª©í‘œì´ë‹¤.</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>\( \mathbf{y}_1 \sim \mathcal{N}(\mathbf{z},\boldsymbol{\Sigma}_1) \)</li>
      <li>\( \mathbf{y}_2 \sim \mathcal{N}(\mathbf{z},\boldsymbol{\Sigma}_2) \)</li>
      <li>ìœ„ì™€ ê°™ì€ ë‘ ì„¼ì„œë¡œë¶€í„° ë°ì´í„°ë¥¼ ì–»ê²Œ ë˜ë©°, ì´ë¥¼ concatenationí•´ì„œ í•˜ë‚˜ì˜ ë²¡í„° \( \mathbf{y} = [\mathbf{y}_1, \mathbf{y}_2] \)ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</li>
    </ul>
  </li>
  <li>\( p(\mathbf{y}|\mathbf{z}) = \mathcal{N}(\mathbf{y}|\mathbf{W}\mathbf{z}, \boldsymbol{\Sigma}_y) \)ì™€ ê°™ì€ í˜•íƒœë¡œ likelihoodë¥¼ í‘œí˜„í•  ìˆ˜ ìˆê²Œ ë˜ëŠ”ë°, ì´ ë•Œ
    <ul>
      <li>\( \mathbf{W} = [\mathbf{I}; \mathbf{I}] \)</li>
      <li>\( \boldsymbol{\Sigma}_y = [\boldsymbol{\Sigma}_1, \mathbf{0}; \mathbf{0}, \boldsymbol{\Sigma}_2]) \)</li>
    </ul>
  </li>
  <li>ì´ì œ ì‹ (3.103)ê³¼ ì‹ (3.104)ë¥¼ ë°”ë¡œ ì´ìš©í•´ posteriorë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.</li>
  <li>ì¼ë‹¨ \( \boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_2 = 0.01 \mathbf{I} \) ì¸ ê²½ìš°ë¥¼ ì˜ˆë¡œ ë“¤ì–´ë³´ì.
    <ul>
      <li>ì¶”ê°€ì ì¸ assumptionìœ¼ë¡œ, \( \boldsymbol{\Sigma}_z = 0.01 \mathbf{I} \)ë„ ê°€ì •í•˜ì.</li>
      <li>ì•„ë˜ì™€ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤.</li>
    </ul>
  </li>
</ul>

\[\begin{align}
\boldsymbol{\Sigma}_{z|y}^{-1} &amp;= \boldsymbol{\Sigma}_{z}^{-1} + \begin{pmatrix}
\mathbf{I} \\ \mathbf{I}\end{pmatrix}^\top \begin{pmatrix} \boldsymbol{\Sigma}_{1}^{-1} &amp; \mathbf{0} \\ \mathbf{0} &amp; \boldsymbol{\Sigma}_2^{-1}\end{pmatrix} \begin{pmatrix} \mathbf{I} \\ \mathbf{I} \end{pmatrix}\\
&amp;= \boldsymbol{\Sigma}_z^{-1} + \boldsymbol{\Sigma}_1^{-1} + \boldsymbol{\Sigma}_2^{-1} \\
&amp;= 300 \mathbf{I}
\end{align}\]

\[\begin{align}
\boldsymbol{\mu}_{z|y} &amp;= 
\boldsymbol{\Sigma}_{z|y} \begin{pmatrix}
\mathbf{I} \\ \mathbf{I} \end{pmatrix}^\top \begin{pmatrix} \boldsymbol{\Sigma}_1^{-1} &amp; \mathbf{0} \\ \mathbf{0} &amp; \boldsymbol{\Sigma}_2^{-1} \end{pmatrix} \begin{pmatrix} \mathbf{y}_1 \\ \mathbf{y}_2 \end{pmatrix}\\
&amp;= 0.03 \mathbf{I} (100 \mathbf{y}_1 + 100 \mathbf{y}_2)
\end{align}\]

<ul>
  <li>ê²°êµ­ ë¶„ì‚°ê°’ì€ ê° sensorì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì‹ ë¢°ë„ê°€ ìˆì—ˆëŠ”ì§€ì— ë”°ë¼ additiveí•˜ê²Œ ì¦ê°€í•˜ê²Œ ë˜ê³ , í‰ê· ê°’ì€ ê° sensor í‰ê· ì„ ì‹ ë¢°ë„ì— ë”°ë¼ weighted sumì„ í•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ê²Œ ëœë‹¤.</li>
</ul>
:ET