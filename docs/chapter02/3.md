---
layout: page-sidenav
group: "Chapter. 2"
title: "3. Bayesian concept learning"
author: Hyungjin Chung
---

- 지난 섹션에서는 hidden quantity \\( h \in \mathcal{H} \\) 를 단 하나의 observation \\( y \\) 로부터 유추하는 방법을 배웠다.
- 이를 일반화하면 observation이 하나가 아닌 여러 개로 이루어진 *set* 일 때를 생각할 수 있다. \\( \mathcal{D} = \{ y_n : n = 1:N \} \\)
- 이제 목표는 주어진 데이터로부터 보편적인 특성(**concept**)을 찾는 일이다.
- 일반적으로 머신러닝을 통해 binary classification을 학습 시킬 때는 positive example과 negative example이 **둘 다 충분히 많이** 필요하다.
- 반면, 아이가 "강아지" 라는 컨셉을 처음 배울 때는 **positive example**만을 갖고도 효율적으로 학습한다.

### 1. Learning a discrete concept: the number game

- 우리가 어떤 수학적인 컨셉을 배운다고 가정하자. 편의를 위해 몇 가지 assumption이 필요하다.
  - \\( h_{even} = \{ 2, 4, 6, \dots \} \\) 와 같이 컨셉은 **extension**으로 설명된다.
  - 숫자는 1부터 100 사이의 숫자이다.
- \\( \mathcal{D} = \{ 2, 8, 16, 64 \} \\) 가 주어지면, 많은 사람들이 일반적으로 이 set은 2의 승수로 이루어져있다고 판단할 것이다.
- 이는 **generalization**의 한 예시이며, \\( \mathcal{D} = \{ 16 \} \\) 와 같이 데이터 하나만 주어졌을 때보다 데이터가 여러 개 주어졌을 때 더 정확히 일반화할 수 있고, **underlying pattern**
을 더 쉽게 찾을 수 있다.
<div class="text-center">
  <img src="{{ site.baseurl }}/images/2.2.PNG" alt="Figure 2.2" height="300px" />
</div>
- 위 그림을 살펴보자. 첫 두 row에서 확인할 수 있듯이, 16과 60만을 본 뒤에는 다른 숫자를 고를 확률이 고르게 분포가 되어있지만, 4개의 숫자를 본 뒤에는 대체로 확률값들이 밀집되어 분포해있다.
- 이러한 **induction** problem을 수학적으로 설명하기 위해 hypothesis space \\( \mathcal{H} \\)가 있다고 가정한다.
  - 짝수, 1과 10 사이의 모든 수 와 같은 것이 하나의 *concept*에 해당한다.
- 그 후, hypothesis space \\( mathcal{H} \\) 에 존재하는 concept 중 observed data \\( \mathcal{D} \\)와 consistent하면서, 가장 크기가 작은 subset을 찾는다. 이를 **version space**
라고 칭한다.
- 그러나, version space theory를 이용해도 사람의 행동을 완전히 설명하기는 어렵다. 모든 짝수, 32를 제외한 모든 2의 승수 등의 subset도 모두 observation과 consistent하지만 사람들은능하다.
그러한 concept을 잘 제시하지 않는다. 이러한 현상 또한 bayesian inference로 설명이 가능하다.

\\
#### 1.1 Likelihood
\\
- 왜 사람들은 \\( \mathcal{D} = \{ 2, 8, 16, 64 \} \\) 를 보고 짝수가 아닌 2의 승수를 먼저 떠올릴까?
- **suspicious coincidences**를 막기 위함이다. 실제 \\( h \\) 가 짝수였다면 왜 2의 승수만 관측됐을까?
- 이를 formal하게 표현하기 위해 observation(example)들이 concept의 extension으로부터 랜덤하게 샘플링 되었다고 생각해보자 (이를 **strong sampling assumption**이라 한다.)
- 이 때, unknown concept \\( h \\)로부터 아이템을 \\( N \\)개 sampling 하는 확률은 아래와 같다.

$$
p(\mathcal{D}|h) = \prod_{n=1}^N p(y_n|h) = \prod_{n=1}^N \frac{1}{\textrm{size}(h)}\mathbb{I}(y_n \in h) = [\frac{1}{\textrm{size}(h)}]^N \mathbb{I}(\mathcal{D} \in h) \qquad{(2.16)}
$$

- observation의 모든 데이터 포인트가 \\( h \\)에 존재하여야 \\( \mathbb{I}(\mathcal{D} \in h) \\)가 0이 아닌 값을 가지며, 따라서 probability는 size가 작으면 작을수록 더 probable하다.
- 이를 **size principle**이라고 하고, **Occam's razor**로 설명할 수도 있다.
- \\( \mathcal{D} = \{ 2, 8, 16, 64 \} \\) 일 때 \\( h_{two} \\) 일 확률은 \\( (1/6)^4  7.7 \times 10 \\) 인데에 비해, \\( h_{even} \\)일 확률은 \\( (1/50)^4 = 1.6 \times 10^{-7} \\) 에 불과하다.
\\
#### 1.2 Prior
\\
- Bayesian approach로 계산하려면 likelihood 뿐만이 아니라 각 hypothesis에 대한 믿음의 정도인 prior \\( p(h) \\)를 알아야 한다.
- 예컨대, \\( \mathcal{D} = \{16, 8, 2, 64\} \\)가 "32를 제외한 2의 승수"가 아니라 "2의 승수"에 해당하는 것이 더 가능성이 높다고 생각하는 데에는 그 이유를 prior에서 찾을 수 있다. Prior를 설정하는 데에는 어느 정도의 **subjectivess**가 포함될 수밖에 없다.
  - 하지만 많은 경우에 prior는 "일반적으로 많은 사람들이 받아들일 수 있도록" 설정할 수 있으며, 위의 경우에는 "2의 승수"가 "32를 제외한 2의 승수"보다 훨씬 더 큰 prior probability를 가질 것이다.
  - 이렇게 prior를 설정함으로써 (**background knowledge**를 끌어들임으로써) 빠른 학습이 가능해진다.
- 지금 이용하고 있는 예제에서는 hypothesis space를 30개로 제한한다.
  1. Rule-like hypothesis
    - "even numbers", "prime numbers"와 같이 **term**으로 정의할 수 있는경우
    - "powers of 2, plus 37"과 같이 직관적으로 unnatural한 term도 포함하고 다만 낮은 prior를 설정한다.
  2. Interval hypothesis
    - \\(a \\) 이상 \\(b) \\) 이하와 같이 특정 interval에도 어느정도의 prior value를 준다.

$$
p(h) = \pi \textrm{Unif}(h|\textrm{rules}) + (1 - \pi)\textrm{Unif}(h|\textrm{intervals}) \\qquad{(2.17)}
$$

-합쳐서, 위와같은 **mixture distribution**을 정의할 수 있다.
\\
#### 1.3 Posterior
\\
- Likelihood과 prior를 설정했으니, posterior를 구할 수 있다.
<div class="text-center">
  <img src="{{ site.baseurl }}/images/2.4.PNG" alt="Figure 2.4" height="300px" />
</div>
- (a)는 \\( \mathcal{D} = \{16\} \\) 일 때의 prior/likelihood/posterior 값이고, (b)는 \\( \mathcal{D} = \{2, 8, 16, 64\} \\) 가 주어진 이후의 counterpart이다.
  - prior가 hypothesis space를 많이 줄여주기는 하지만, (a)에서는 그래도 posterior에 다양한 hypothesis가 가능한 반면, (b)에서는 아주 작은 hypothesis space만이 남는다.
  - 중요하게 확인할 수 있는 것은, "32를 제외한 2의 승수"와 같이 직관적으로 improable한 hypothesis들은 높은 likelihood값을 가지지만, **prior 값이 작기** 때문에 **작은 posterior** 값을 갖는다.
  - 이 예시에서 왜 "32를 제외한 2의 승수"와 같이 unnatural한 컨셉에 low prior를 줘야 하는지를 알 수 있다. 만약 높은 prior를 줬다면 overfitting으로 posterior값이 가장 큰 hypotehsis는 "32를 제외한 2의 승수"가 되었을 것이다.

#### 1.4 Posterior predictive 

